# Lexicon Agent Framework ä½¿ç”¨æŒ‡å—

## æ¡†æ¶æ¦‚è¿°

Lexicon Agent Framework æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œä¸“ä¸ºæ„å»ºå¤æ‚çš„æ™ºèƒ½ç³»ç»Ÿè€Œè®¾è®¡ã€‚è¯¥æ¡†æ¶æä¾›äº†å®Œæ•´çš„æ™ºèƒ½ä½“ç¼–æ’ã€ä¸Šä¸‹æ–‡ç®¡ç†ã€å·¥å…·è°ƒç”¨å’Œæµå¼å¤„ç†èƒ½åŠ›ã€‚

### æ ¸å¿ƒç‰¹æ€§

ğŸ¤– **å¤šæ™ºèƒ½ä½“ç¼–æ’**ï¼šæ”¯æŒå¤šä¸ªæ™ºèƒ½ä½“çš„åè°ƒå’Œåä½œ
ğŸ§  **æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šè‡ªåŠ¨ç®¡ç†å’Œä¼˜åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯
ğŸ”§ **ä¸°å¯Œçš„å·¥å…·ç³»ç»Ÿ**ï¼šå†…ç½®æ–‡ä»¶æ“ä½œã€çŸ¥è¯†åº“ã€ä»£ç æ‰§è¡Œç­‰å·¥å…·
ğŸŒŠ **æµå¼æ•°æ®å¤„ç†**ï¼šæ”¯æŒå®æ—¶æ•°æ®æµå¤„ç†å’Œå“åº”
ğŸ”’ **å®‰å…¨æœºåˆ¶**ï¼šå†…ç½®å®‰å…¨æ£€æŸ¥å’Œæƒé™æ§åˆ¶
âš¡ **é«˜æ€§èƒ½**ï¼šå¼‚æ­¥æ¶æ„ï¼Œæ”¯æŒå¹¶å‘å¤„ç†
ğŸ”Œ **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€å¯¼å…¥

```python
import asyncio
from lexicon_agent.types import Agent, SessionState, ContextRequirements
from lexicon_agent.core.tools.registry import ToolRegistry
from lexicon_agent.core.orchestration.engine import OrchestrationEngine, UserInput, OrchestrationContext
```

### 2. åˆ›å»ºæ™ºèƒ½ä½“

```python
# åˆ›å»ºæ™ºèƒ½ä½“
agent = Agent(
    agent_id="my_agent_001",
    name="æ•°æ®åˆ†ææ™ºèƒ½ä½“",
    specialization="data_analysis",
    capabilities=["data_analysis", "file_operations", "reporting"],
    status="available",
    configuration={"max_concurrent_tasks": 3}
)
```

### 3. åˆå§‹åŒ–å·¥å…·ç³»ç»Ÿ

```python
# è·å–å·¥å…·æ³¨å†Œè¡¨
tool_registry = ToolRegistry()

# æŸ¥çœ‹å¯ç”¨å·¥å…·
available_tools = tool_registry.list_tools()
print(f"å¯ç”¨å·¥å…·: {available_tools}")

# ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå·¥å…·
fs_tool = tool_registry.get_tool("file_system")
result = await fs_tool.execute({
    "action": "read",
    "path": "data.txt"
})
```

### 4. ç¼–æ’æ‰§è¡Œ

```python
# åˆ›å»ºç¼–æ’å¼•æ“
engine = OrchestrationEngine()

# ç”¨æˆ·è¾“å…¥
user_input = UserInput(
    message="è¯·åˆ†ææ•°æ®æ–‡ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š",
    context={"task_type": "data_analysis"}
)

# åˆ›å»ºç¼–æ’ä¸Šä¸‹æ–‡
context = OrchestrationContext(
    user_input=user_input,
    available_agents=[agent],
    session_context={"project": "sales_analysis"}
)

# æ‰§è¡Œç¼–æ’
result = await engine.orchestrate(user_input, [agent], context)
```

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. æ™ºèƒ½ä½“ç³»ç»Ÿ (Agent System)

#### æ™ºèƒ½ä½“å®šä¹‰

æ™ºèƒ½ä½“æ˜¯æ¡†æ¶çš„æ ¸å¿ƒæ‰§è¡Œå•å…ƒï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å…·æœ‰ç‰¹å®šçš„èƒ½åŠ›å’Œä¸“ä¸šåŒ–é¢†åŸŸï¼š

```python
from lexicon_agent.types import Agent

agent = Agent(
    agent_id="unique_id",           # å”¯ä¸€æ ‡è¯†ç¬¦
    name="æ™ºèƒ½ä½“åç§°",              # æ˜¾ç¤ºåç§°
    specialization="domain",        # ä¸“ä¸šåŒ–é¢†åŸŸ
    capabilities=[                  # èƒ½åŠ›åˆ—è¡¨
        "data_analysis",
        "code_generation",
        "file_operations"
    ],
    status="available",             # çŠ¶æ€ï¼šavailable/busy/offline
    configuration={                 # é…ç½®å‚æ•°
        "max_tasks": 5,
        "timeout": 30
    }
)
```

#### æ™ºèƒ½ä½“ç®¡ç†

```python
from lexicon_agent.core.agent.controller import AgentController
from lexicon_agent.core.context import ContextRetrievalEngine, ContextProcessor, ContextManager

# åˆå§‹åŒ–ä¾èµ–ç»„ä»¶
context_engine = ContextRetrievalEngine()
context_processor = ContextProcessor()
context_manager = ContextManager()

# åˆ›å»ºæ™ºèƒ½ä½“æ§åˆ¶å™¨
controller = AgentController(
    context_engine=context_engine,
    context_processor=context_processor,
    context_manager=context_manager
)

# æ³¨å†Œæ™ºèƒ½ä½“
await controller.register_agent(agent)

# ç”Ÿæˆå“åº”æµ
async for event in controller.generate_response(user_input, session_context):
    if event.type == "response_delta":
        print(event.data["content"], end="")
```

### 2. ä¸Šä¸‹æ–‡ç®¡ç† (Context Management)

#### ä¼šè¯çŠ¶æ€ç®¡ç†

```python
from lexicon_agent.types import SessionState

session_state = SessionState(
    session_id="session_001",
    context_memory={                # ä¸Šä¸‹æ–‡è®°å¿†
        "user_preferences": {"format": "json"},
        "previous_results": []
    },
    conversation_history=[          # å¯¹è¯å†å²
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"}
    ],
    environment_state={             # ç¯å¢ƒçŠ¶æ€
        "working_directory": "/path/to/work",
        "available_resources": ["cpu", "memory"]
    }
)
```

#### ä¸Šä¸‹æ–‡æ£€ç´¢

```python
from lexicon_agent.types import ContextRequirements
from lexicon_agent.core.context.management import ContextManager

# å®šä¹‰ä¸Šä¸‹æ–‡éœ€æ±‚
requirements = ContextRequirements(
    strategies=["semantic_search", "rule_based"],
    max_tokens=2000,
    priorities=["relevance", "recency"]
)

# ç®¡ç†ä¸Šä¸‹æ–‡
context_manager = ContextManager()
managed_context = await context_manager.manage_context(
    session_state, 
    requirements
)
```

### 3. å·¥å…·ç³»ç»Ÿ (Tool System)

#### å†…ç½®å·¥å…·

æ¡†æ¶æä¾›å¤šç§å†…ç½®å·¥å…·ï¼š

**æ–‡ä»¶ç³»ç»Ÿå·¥å…·**
```python
fs_tool = tool_registry.get_tool("file_system")

# è¯»å–æ–‡ä»¶
result = await fs_tool.execute({
    "action": "read",
    "path": "document.txt"
})

# å†™å…¥æ–‡ä»¶
result = await fs_tool.execute({
    "action": "write",
    "path": "output.txt",
    "content": "Hello, World!"
})

# åˆ—å‡ºç›®å½•
result = await fs_tool.execute({
    "action": "list",
    "path": "/path/to/directory"
})
```

**çŸ¥è¯†åº“å·¥å…·**
```python
kb_tool = tool_registry.get_tool("knowledge_base")

# åˆ›å»ºçŸ¥è¯†åº“
await kb_tool.execute({
    "action": "create",
    "kb_name": "my_knowledge_base",
    "description": "é¡¹ç›®æ–‡æ¡£çŸ¥è¯†åº“"
})

# æ·»åŠ æ–‡æ¡£
await kb_tool.execute({
    "action": "add",
    "kb_name": "my_knowledge_base",
    "title": "APIæ–‡æ¡£",
    "text": "è¿™æ˜¯APIä½¿ç”¨è¯´æ˜...",
    "metadata": {"category": "documentation"}
})

# æœç´¢æ–‡æ¡£
result = await kb_tool.execute({
    "action": "search",
    "kb_name": "my_knowledge_base",
    "query": "APIä½¿ç”¨æ–¹æ³•",
    "limit": 5
})
```

**ä»£ç è§£é‡Šå™¨å·¥å…·**
```python
code_tool = tool_registry.get_tool("code_interpreter")

# æ‰§è¡ŒPythonä»£ç 
result = await code_tool.execute({
    "language": "python",
    "code": """
def hello_world():
    return "Hello, World!"

print(hello_world())
    """,
    "timeout": 10
})
```

#### è‡ªå®šä¹‰å·¥å…·

```python
from lexicon_agent.core.tools.registry import BaseTool
from lexicon_agent.types import ToolSafetyLevel

class CustomTool(BaseTool):
    def __init__(self):
        super().__init__(
            name="custom_tool",
            description="è‡ªå®šä¹‰å·¥å…·ç¤ºä¾‹"
        )
    
    async def execute(self, input_data: dict) -> dict:
        # å®ç°å·¥å…·é€»è¾‘
        return {
            "result": "Custom tool executed successfully",
            "input_received": input_data,
            "success": True
        }
    
    def get_input_schema(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "parameter": {"type": "string"}
            },
            "required": ["parameter"]
        }
    
    def get_safety_level(self) -> ToolSafetyLevel:
        return ToolSafetyLevel.SAFE

# æ³¨å†Œè‡ªå®šä¹‰å·¥å…·
custom_tool = CustomTool()
tool_registry.register_tool(custom_tool)
```

### 4. æµå¼å¤„ç† (Streaming)

#### åˆ›å»ºå’Œç®¡ç†æµ

```python
from lexicon_agent.core.streaming.processor import StreamingProcessor

processor = StreamingProcessor()

# åˆ›å»ºæµ
stream_id = "data_stream_001"
success = await processor.create_stream(stream_id, "json")

# å¤„ç†æµæ•°æ®
async for chunk in processor.process_stream(stream_id, data_iterator):
    print(f"å¤„ç†å—: {chunk.data}")

# æ¸…ç†æµ
await processor._finalize_stream(stream_id, processor.processors["json"])
```

#### æµå¼ç®¡é“

```python
from lexicon_agent.core.streaming.pipeline import StreamingPipeline

pipeline = StreamingPipeline()

# åˆ›å»ºå¤„ç†ç®¡é“
await pipeline.create_pipeline(
    pipeline_id="analysis_pipeline",
    stages=[
        {"name": "data_input", "processor": "json"},
        {"name": "analysis", "processor": "text"},
        {"name": "output", "processor": "event"}
    ]
)

# æ‰§è¡Œç®¡é“
async for result in pipeline.execute_pipeline("analysis_pipeline", input_data):
    print(f"ç®¡é“è¾“å‡º: {result}")
```

### 5. ç¼–æ’å¼•æ“ (Orchestration)

#### åŸºç¡€ç¼–æ’

```python
from lexicon_agent.core.orchestration.engine import OrchestrationEngine, UserInput, OrchestrationContext

engine = OrchestrationEngine()

# åˆ›å»ºç”¨æˆ·è¾“å…¥
user_input = UserInput(
    message="è¯·åˆ†æé”€å”®æ•°æ®å¹¶ç”Ÿæˆæœˆåº¦æŠ¥å‘Šï¼ŒåŒ…æ‹¬è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹",
    context={"department": "sales", "period": "monthly"}
)

# åˆ›å»ºç¼–æ’ä¸Šä¸‹æ–‡
context = OrchestrationContext(
    user_input=user_input,
    available_agents=[data_agent, report_agent],
    session_context={"user_id": "user123"},
    constraints={"max_execution_time": 300}
)

# æ‰§è¡Œç¼–æ’
result = await engine.orchestrate(user_input, available_agents, context)
```

#### å·¥å…·ç¼–æ’

```python
from lexicon_agent.types import ToolCall, ManagedContext

# å‡†å¤‡å·¥å…·è°ƒç”¨
tool_calls = [
    ToolCall(
        tool_name="file_system",
        call_id="read_data",
        parameters={"action": "read", "path": "sales_data.csv"}
    ),
    ToolCall(
        tool_name="code_interpreter", 
        call_id="analyze_data",
        parameters={
            "language": "python",
            "code": "import pandas as pd; df = pd.read_csv('sales_data.csv'); print(df.describe())"
        }
    )
]

# åˆ›å»ºç®¡ç†ä¸Šä¸‹æ–‡
managed_context = ManagedContext(
    session_id="analysis_session",
    constraints={"memory_limit": "1GB"},
    metadata={"task": "data_analysis"}
)

# æ‰§è¡Œå·¥å…·ç¼–æ’
async for event in engine.orchestrate_tools(tool_calls, managed_context):
    print(f"å·¥å…·æ‰§è¡Œäº‹ä»¶: {event}")
```

## é«˜çº§åŠŸèƒ½

### 1. LLM é›†æˆ

æ¡†æ¶æ”¯æŒå¤šç§LLM APIçš„é›†æˆï¼š

```python
import os
import aiohttp

class LLMIntegration:
    def __init__(self):
        self.api_key = os.getenv('LLM_API_KEY')
        self.base_url = os.getenv('LLM_BASE_URL', 'https://api.openai.com/v1')
        self.model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
    
    async def generate_response(self, prompt: str) -> dict:
        async with aiohttp.ClientSession() as session:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 500
            }
            
            async with session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            ) as response:
                return await response.json()

# ä½¿ç”¨ç¤ºä¾‹
llm = LLMIntegration()
response = await llm.generate_response("è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†")
```

### 2. é…ç½®ç®¡ç†

```python
from lexicon_agent.config import ConfigurationManager

# åŠ è½½é…ç½®
config_manager = ConfigurationManager()
await config_manager.load_configuration("config.yaml")

# è·å–é…ç½®
db_config = config_manager.get_config_section("database")
llm_config = config_manager.get_config_section("llm")

# åŠ¨æ€æ›´æ–°é…ç½®
await config_manager.update_config("llm.model", "gpt-4")
```

### 3. æ€§èƒ½ç›‘æ§

```python
# è·å–ç³»ç»ŸæŒ‡æ ‡
metrics = await engine.get_orchestration_status()
print(f"æ´»è·ƒæµç¨‹: {metrics['active_flows']}")
print(f"æ€§èƒ½æŒ‡æ ‡: {metrics['performance_metrics']}")

# å·¥å…·ä½¿ç”¨ç»Ÿè®¡
tool_metrics = tool_registry.get_registry_statistics()
print(f"å·¥å…·æˆåŠŸç‡: {tool_metrics['overall_success_rate']}")
```

## éƒ¨ç½²å’Œè¿è¡Œ

### 1. å®‰è£…ä¾èµ–

```bash
pip install aiohttp pydantic python-dotenv
```

### 2. ç¯å¢ƒé…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# LLM APIé…ç½®
LLM_API_KEY=your_api_key_here
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-3.5-turbo

# æ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼‰
DATABASE_URL=sqlite:///lexicon.db

# æ—¥å¿—çº§åˆ«
LOG_LEVEL=INFO
```

### 3. åŸºç¡€åº”ç”¨ç¤ºä¾‹

```python
import asyncio
from lexicon_agent.main import main

async def run_application():
    """è¿è¡ŒLexicon Agentåº”ç”¨"""
    
    # åˆå§‹åŒ–ç»„ä»¶
    tool_registry = ToolRegistry()
    engine = OrchestrationEngine()
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    assistant_agent = Agent(
        agent_id="assistant_001",
        name="é€šç”¨åŠ©æ‰‹",
        specialization="general",
        capabilities=["file_operations", "data_analysis", "code_generation"],
        status="available"
    )
    
    # æ³¨å†Œæ™ºèƒ½ä½“å’Œå¯åŠ¨æœåŠ¡
    available_agents = [assistant_agent]
    
    # ä¸»å¾ªç¯
    while True:
        user_message = input("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ (è¾“å…¥ 'quit' é€€å‡º): ")
        if user_message.lower() == 'quit':
            break
        
        # åˆ›å»ºç”¨æˆ·è¾“å…¥
        user_input = UserInput(message=user_message)
        
        # åˆ›å»ºç¼–æ’ä¸Šä¸‹æ–‡
        context = OrchestrationContext(
            user_input=user_input,
            available_agents=available_agents
        )
        
        # æ‰§è¡Œç¼–æ’
        result = await engine.orchestrate(user_input, available_agents, context)
        
        # è¾“å‡ºç»“æœ
        print(f"åŠ©æ‰‹å›å¤: {result.primary_result}")
        print(f"å‚ä¸æ™ºèƒ½ä½“: {len(result.participating_agents)}")
        print("-" * 50)

if __name__ == "__main__":
    asyncio.run(run_application())
```

## æœ€ä½³å®è·µ

### 1. æ™ºèƒ½ä½“è®¾è®¡

- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ä¸“æ³¨äºç‰¹å®šé¢†åŸŸ
- **èƒ½åŠ›æ˜ç¡®**ï¼šæ¸…æ™°å®šä¹‰æ™ºèƒ½ä½“çš„èƒ½åŠ›è¾¹ç•Œ
- **çŠ¶æ€ç®¡ç†**ï¼šåŠæ—¶æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€

### 2. ä¸Šä¸‹æ–‡ç®¡ç†

- **é€‚é‡ä¸Šä¸‹æ–‡**ï¼šé¿å…ä¸Šä¸‹æ–‡è¿‡è½½
- **å®šæœŸæ¸…ç†**ï¼šæ¸…ç†è¿‡æœŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **ä¼˜å…ˆçº§ç®¡ç†**ï¼šåˆç†è®¾ç½®ä¸Šä¸‹æ–‡ä¼˜å…ˆçº§

### 3. å·¥å…·ä½¿ç”¨

- **å®‰å…¨ç¬¬ä¸€**ï¼šå§‹ç»ˆè€ƒè™‘å·¥å…·çš„å®‰å…¨çº§åˆ«
- **é”™è¯¯å¤„ç†**ï¼šå®ç°å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶
- **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§å·¥å…·æ‰§è¡Œæ€§èƒ½

### 4. æµå¼å¤„ç†

- **èµ„æºç®¡ç†**ï¼šåŠæ—¶æ¸…ç†ä¸ç”¨çš„æµ
- **å¹¶å‘æ§åˆ¶**ï¼šåˆç†æ§åˆ¶å¹¶å‘æµæ•°é‡
- **é”™è¯¯æ¢å¤**ï¼šå®ç°æµå¤„ç†é”™è¯¯æ¢å¤

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ™ºèƒ½ä½“æ³¨å†Œå¤±è´¥**
   - æ£€æŸ¥æ™ºèƒ½ä½“IDæ˜¯å¦å”¯ä¸€
   - éªŒè¯å¿…éœ€å­—æ®µæ˜¯å¦å®Œæ•´

2. **å·¥å…·æ‰§è¡Œé”™è¯¯**
   - æ£€æŸ¥è¾“å…¥å‚æ•°æ ¼å¼
   - éªŒè¯å·¥å…·æƒé™è®¾ç½®

3. **ä¸Šä¸‹æ–‡ç®¡ç†é—®é¢˜**
   - æ£€æŸ¥ä¸Šä¸‹æ–‡å¤§å°é™åˆ¶
   - éªŒè¯ä¸Šä¸‹æ–‡æ ¼å¼æ­£ç¡®æ€§

4. **æµå¼å¤„ç†å¼‚å¸¸**
   - æ£€æŸ¥æµIDæ˜¯å¦å†²çª
   - éªŒè¯å¤„ç†å™¨ç±»å‹åŒ¹é…

### è°ƒè¯•æŠ€å·§

```python
import logging

# å¯ç”¨è°ƒè¯•æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# æŸ¥çœ‹ç»„ä»¶çŠ¶æ€
print("å·¥å…·çŠ¶æ€:", tool_registry.get_registry_statistics())
print("ç¼–æ’çŠ¶æ€:", await engine.get_orchestration_status())
```

## æ‰©å±•å¼€å‘

### 1. è‡ªå®šä¹‰æ™ºèƒ½ä½“

```python
class SpecializedAgent(Agent):
    def __init__(self, domain_config):
        super().__init__(
            agent_id=f"specialized_{domain_config['domain']}",
            name=f"{domain_config['domain']}ä¸“å®¶",
            specialization=domain_config['domain'],
            capabilities=domain_config['capabilities']
        )
        self.domain_config = domain_config
    
    async def specialized_operation(self, task):
        # å®ç°ä¸“ä¸šåŒ–æ“ä½œ
        pass
```

### 2. è‡ªå®šä¹‰ç¼–æ’ç­–ç•¥

```python
from lexicon_agent.core.orchestration.strategies import OrchestrationStrategy

class CustomStrategy(OrchestrationStrategy):
    async def determine_agent_count(self, task_requirements):
        # è‡ªå®šä¹‰æ™ºèƒ½ä½“æ•°é‡é€»è¾‘
        return min(3, task_requirements.get("complexity", 1))
    
    async def create_execution_plan(self, agents, context):
        # è‡ªå®šä¹‰æ‰§è¡Œè®¡åˆ’
        return {
            "strategy": "custom",
            "steps": ["analyze", "execute", "verify"],
            "parallel_execution": True
        }
```

## æ€»ç»“

Lexicon Agent Framework æä¾›äº†æ„å»ºå¤æ‚æ™ºèƒ½ç³»ç»Ÿæ‰€éœ€çš„å®Œæ•´åŸºç¡€è®¾æ–½ã€‚é€šè¿‡åˆç†ä½¿ç”¨æ¡†æ¶çš„å„ä¸ªç»„ä»¶ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿæ„å»ºå‡ºåŠŸèƒ½å¼ºå¤§ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ç³»ç»Ÿã€‚

æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ç¡®ä¿äº†é«˜åº¦çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒåŒæ—¶å†…ç½®çš„å®‰å…¨æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŒ–ä¿è¯äº†ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

å¼€å§‹ä½¿ç”¨æ¡†æ¶æ—¶ï¼Œå»ºè®®ä»ç®€å•çš„å•æ™ºèƒ½ä½“åœºæ™¯å¼€å§‹ï¼Œé€æ­¥æ‰©å±•åˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿã€‚