"""
ä½¿ç”¨çœŸå®LLMæµ‹è¯• - è¯¦ç»†è¾“å‡ºç‰ˆæœ¬
æ˜¾ç¤ºAgentçš„æ€è€ƒè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨ç»†èŠ‚
"""
import asyncio
import os
from loom import LoomBuilder
from loom.llm import OpenAIProvider
from loom.kernel.core import UniversalEventBus, Dispatcher
from loom.protocol.cloudevents import CloudEvent
from loom.node.tool import ToolNode
from loom.protocol.mcp import MCPToolDefinition

# è®¾ç½®OpenAIå‡­è¯
os.environ["OPENAI_API_KEY"] = "sk-Fy6Y5WV5eugN61DhxH1AjI8th71OWfopqA2OCj5t93UIZ6aF"
os.environ["OPENAI_MODEL"] = "gpt-4o-mini"
os.environ["OPENAI_BASE_URL"] = "https://xiaoai.plus/v1"

# äº‹ä»¶ç›‘å¬å™¨
class VerboseEventListener:
    """è¯¦ç»†è¾“å‡ºçš„äº‹ä»¶ç›‘å¬å™¨"""

    def __init__(self):
        self.iteration = 0

    async def on_event(self, event: CloudEvent):
        """å¤„ç†æ‰€æœ‰äº‹ä»¶"""
        event_type = event.type

        if event_type == "agent.tool.call":
            # å·¥å…·è°ƒç”¨äº‹ä»¶
            data = event.data or {}
            tool_name = data.get("tool", "unknown")
            args = data.get("arguments", {})
            print(f"\nğŸ”§ [å·¥å…·è°ƒç”¨] {tool_name}")
            print(f"   å‚æ•°: {args}")

        elif event_type == "agent.tool.result":
            # å·¥å…·ç»“æœäº‹ä»¶
            data = event.data or {}
            tool_name = data.get("tool", "unknown")
            result = data.get("result", "")
            print(f"âœ… [å·¥å…·ç»“æœ] {tool_name}")
            print(f"   è¿”å›: {result}")

print("=" * 60)
print("æµ‹è¯•: çœŸå®LLM + è¯¦ç»†è¾“å‡º")
print("=" * 60)

async def test_with_verbose_output():
    """å¸¦è¯¦ç»†è¾“å‡ºçš„æµ‹è¯•"""
    print("\nğŸ”§ åˆ›å»ºåŸºç¡€è®¾æ–½...")
    bus = UniversalEventBus()
    dispatcher = Dispatcher(bus)

    # åˆ›å»ºäº‹ä»¶ç›‘å¬å™¨
    listener = VerboseEventListener()

    # æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
    async def event_handler(event: CloudEvent):
        await listener.on_event(event)

    # è®¢é˜…æ‰€æœ‰agentç›¸å…³äº‹ä»¶
    await bus.subscribe("agent.tool.call", event_handler)
    await bus.subscribe("agent.tool.result", event_handler)

    print("âœ… äº‹ä»¶ç›‘å¬å™¨å·²æ³¨å†Œ")

    # åˆ›å»ºOpenAI Provider
    print("\nğŸ”§ åˆ›å»ºOpenAI Provider...")
    provider = OpenAIProvider(
        api_key=os.environ["OPENAI_API_KEY"],
        model=os.environ["OPENAI_MODEL"],
        base_url=os.environ["OPENAI_BASE_URL"],
        stream=False
    )
    print(f"âœ… Provideråˆ›å»ºæˆåŠŸ: {provider.config.generation.model}")

    # åˆ›å»ºå·¥å…·
    print("\nğŸ”§ åˆ›å»ºè®¡ç®—å™¨å·¥å…·...")
    def calculator(operation: str, a: float, b: float) -> float:
        """è®¡ç®—å™¨å·¥å…·"""
        print(f"   ğŸ’¡ [å·¥å…·æ‰§è¡Œ] calculator({operation}, {a}, {b})")
        if operation == "add":
            result = a + b
        elif operation == "multiply":
            result = a * b
        elif operation == "subtract":
            result = a - b
        elif operation == "divide":
            result = a / b if b != 0 else "Error"
        else:
            result = "Unknown operation"
        print(f"   ğŸ’¡ [å·¥å…·è¿”å›] {result}")
        return result

    tool_def = MCPToolDefinition(
        name="calculator",
        description="A calculator that performs basic arithmetic operations",
        inputSchema={
            "type": "object",
            "properties": {
                "operation": {"type": "string", "enum": ["add", "multiply", "subtract", "divide"]},
                "a": {"type": "number"},
                "b": {"type": "number"}
            },
            "required": ["operation", "a", "b"]
        }
    )

    tool_node = ToolNode(
        node_id="calculator-tool",
        dispatcher=dispatcher,
        tool_def=tool_def,
        func=calculator
    )
    print(f"âœ… å·¥å…·èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {tool_node.node_id}")

    # åˆ›å»ºAgent
    print("\nğŸ”§ åˆ›å»ºAgent...")
    agent = (LoomBuilder()
        .with_id('verbose-agent')
        .with_llm(provider)
        .with_dispatcher(dispatcher)
        .with_tools([tool_node])
        .with_agent(
            role='Math Assistant',
            system_prompt='You are a helpful math assistant. Use the calculator tool to perform calculations.'
        )
        .build())
    print(f"âœ… Agentåˆ›å»ºæˆåŠŸ: {agent.node_id}")
    print(f"   å·²æ³¨å†Œå·¥å…·: {list(agent.known_tools.keys())}")

    # æ‰§è¡Œæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ“¨ å‘é€é—®é¢˜: What is 123 multiplied by 456?")
    print("=" * 60)

    event = CloudEvent(
        type="node.request",
        source="user",
        data={"content": "What is 123 multiplied by 456? Please use the calculator tool."}
    )

    print("\nğŸ¤” [Agentå¼€å§‹æ€è€ƒ...]")
    result = await agent.process(event)

    print("\n" + "=" * 60)
    print("ğŸ¤– [æœ€ç»ˆå“åº”]")
    print("=" * 60)
    print(result)


async def test_stage1_projection():
    """æµ‹è¯•é˜¶æ®µ1ï¼šæŠ•å½±ä¼˜åŒ–ï¼ˆé¢„ç®—æ§åˆ¶ã€è¯­ä¹‰ç›¸å…³æ€§è¯„åˆ†ã€æ¨¡å¼æ£€æµ‹ï¼‰"""
    print("\n" + "=" * 80)
    print("ğŸ§ª é˜¶æ®µ1æµ‹è¯•ï¼šæŠ•å½±ä¼˜åŒ–")
    print("=" * 80)

    from loom.memory.core import LoomMemory
    from loom.memory.types import MemoryUnit, MemoryTier, MemoryType
    from loom.projection.profiles import ProjectionMode

    # åˆ›å»ºLoomMemory
    print("\nğŸ”§ åˆ›å»ºLoomMemory...")
    memory = LoomMemory("test-agent")
    print("âœ… LoomMemoryåˆ›å»ºæˆåŠŸ")

    # æ·»åŠ ä¸€äº›L4 facts
    print("\nğŸ“ æ·»åŠ L4 facts...")
    facts = [
        "Python is a high-level programming language",
        "Machine learning is a subset of artificial intelligence",
        "Neural networks are inspired by biological neurons",
        "Deep learning uses multiple layers of neural networks",
        "Natural language processing deals with text and speech",
        "Computer vision enables machines to interpret images",
        "Reinforcement learning learns through trial and error",
        "Supervised learning uses labeled training data"
    ]

    for i, fact in enumerate(facts):
        await memory.add(MemoryUnit(
            content=fact,
            tier=MemoryTier.L4_GLOBAL,
            type=MemoryType.FACT,
            importance=0.7 + (i * 0.03)
        ))
    print(f"âœ… å·²æ·»åŠ  {len(facts)} ä¸ªfactsåˆ°L4")

    # æ·»åŠ ä¸€ä¸ªè®¡åˆ’åˆ°L2
    print("\nğŸ“‹ æ·»åŠ è®¡åˆ’åˆ°L2...")
    await memory.add(MemoryUnit(
        content="Plan: Research AI technologies and explain key concepts",
        tier=MemoryTier.L2_WORKING,
        type=MemoryType.PLAN,
        importance=0.9
    ))
    print("âœ… è®¡åˆ’å·²æ·»åŠ ")

    # æµ‹è¯•ä¸åŒæ¨¡å¼çš„æŠ•å½±
    test_cases = [
        ("Explain neural networks", "é¢„æœŸ: ANALYTICALæ¨¡å¼"),
        ("Fix the error", "é¢„æœŸ: DEBUGæ¨¡å¼"),
        ("ç»§ç»­ä¹‹å‰çš„è®¨è®º", "é¢„æœŸ: CONTEXTUALæ¨¡å¼"),
        ("æŸ¥è¯¢", "é¢„æœŸ: MINIMALæ¨¡å¼"),
        ("Analyze machine learning algorithms", "é¢„æœŸ: ANALYTICALæ¨¡å¼")
    ]

    print("\n" + "=" * 80)
    print("ğŸ” æµ‹è¯•æ¨¡å¼æ£€æµ‹å’ŒæŠ•å½±")
    print("=" * 80)

    for instruction, expected in test_cases:
        print(f"\nğŸ“¨ æŒ‡ä»¤: '{instruction}'")
        print(f"   {expected}")

        # æ£€æµ‹æ¨¡å¼
        detected_mode = memory._detect_mode(instruction)
        print(f"   âœ… æ£€æµ‹åˆ°æ¨¡å¼: {detected_mode.value}")

        # åˆ›å»ºæŠ•å½±
        projection = await memory.create_projection(
            instruction=instruction,
            total_budget=2000
        )

        print(f"   ğŸ“Š æŠ•å½±ç»“æœ:")
        print(f"      - æŒ‡ä»¤: {projection.instruction}")
        print(f"      - åŒ…å«è®¡åˆ’: {projection.parent_plan is not None}")
        print(f"      - ç›¸å…³factsæ•°é‡: {len(projection.relevant_facts) if projection.relevant_facts else 0}")

        if projection.relevant_facts:
            print(f"      - Top 3 facts:")
            for i, fact in enumerate(projection.relevant_facts[:3]):
                content_preview = str(fact.content)[:50]
                print(f"        {i+1}. {content_preview}... (é‡è¦æ€§: {fact.importance:.2f})")

    print("\nâœ… é˜¶æ®µ1æµ‹è¯•å®Œæˆ")


async def test_stage2_l4_compression():
    """æµ‹è¯•é˜¶æ®µ2ï¼šL4å‹ç¼©ï¼ˆèšç±» + LLMæ€»ç»“ï¼‰"""
    print("\n" + "=" * 80)
    print("ğŸ§ª é˜¶æ®µ2æµ‹è¯•ï¼šL4å‹ç¼©")
    print("=" * 80)

    from loom.memory.core import LoomMemory
    from loom.memory.types import MemoryUnit, MemoryTier, MemoryType
    from loom.llm import OpenAIProvider

    # åˆ›å»ºProvider
    print("\nğŸ”§ åˆ›å»ºOpenAI Provider...")
    provider = OpenAIProvider(
        api_key=os.environ["OPENAI_API_KEY"],
        model=os.environ["OPENAI_MODEL"],
        base_url=os.environ["OPENAI_BASE_URL"],
        stream=False
    )
    print(f"âœ… Provideråˆ›å»ºæˆåŠŸ: {provider.config.generation.model}")

    # åˆ›å»ºLoomMemory
    print("\nğŸ”§ åˆ›å»ºLoomMemory...")
    memory = LoomMemory("test-agent")
    print("âœ… LoomMemoryåˆ›å»ºæˆåŠŸ")

    # å¯ç”¨L4å‹ç¼©ï¼ˆè®¾ç½®ä½é˜ˆå€¼ä»¥ä¾¿æµ‹è¯•ï¼‰
    print("\nğŸ”§ å¯ç”¨L4å‹ç¼©...")
    memory.enable_l4_compression(
        llm_provider=provider,
        threshold=10,  # ä½é˜ˆå€¼ï¼Œæ–¹ä¾¿æµ‹è¯•
        similarity_threshold=0.75,
        min_cluster_size=3
    )
    print("âœ… L4å‹ç¼©å·²å¯ç”¨")
    print(f"   - é˜ˆå€¼: {memory.l4_compressor.threshold}")
    print(f"   - ç›¸ä¼¼åº¦é˜ˆå€¼: {memory.l4_compressor.similarity_threshold}")
    print(f"   - æœ€å°èšç±»å¤§å°: {memory.l4_compressor.min_cluster_size}")

    # æ·»åŠ ç›¸ä¼¼çš„factsï¼ˆä¼šè¢«èšç±»ï¼‰
    print("\nğŸ“ æ·»åŠ ç›¸ä¼¼çš„factsåˆ°L4...")
    similar_facts = [
        # å…³äºPythonçš„factsï¼ˆåº”è¯¥è¢«èšç±»ï¼‰
        "Python is a high-level programming language",
        "Python was created by Guido van Rossum",
        "Python is widely used for data science",
        "Python has a simple and readable syntax",

        # å…³äºæœºå™¨å­¦ä¹ çš„factsï¼ˆåº”è¯¥è¢«èšç±»ï¼‰
        "Machine learning is a subset of AI",
        "Machine learning algorithms learn from data",
        "Machine learning can be supervised or unsupervised",

        # å…³äºç¥ç»ç½‘ç»œçš„factsï¼ˆåº”è¯¥è¢«èšç±»ï¼‰
        "Neural networks are inspired by the brain",
        "Neural networks consist of layers of neurons",
        "Deep neural networks have many hidden layers",

        # å…¶ä»–ç‹¬ç«‹çš„facts
        "JavaScript is used for web development",
        "SQL is used for database queries"
    ]

    for i, fact in enumerate(similar_facts):
        await memory.add(MemoryUnit(
            content=fact,
            tier=MemoryTier.L4_GLOBAL,
            type=MemoryType.FACT,
            importance=0.7 + (i * 0.02)
        ))

    print(f"âœ… å·²æ·»åŠ  {len(similar_facts)} ä¸ªfactsåˆ°L4")
    print(f"   å½“å‰L4å¤§å°: {len(memory._l4_global)}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
    print("\nğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©...")
    should_compress = await memory.l4_compressor.should_compress(memory._l4_global)
    print(f"   éœ€è¦å‹ç¼©: {should_compress}")

    if should_compress:
        print("\nğŸ—œï¸  å¼€å§‹L4å‹ç¼©...")
        print(f"   å‹ç¼©å‰: {len(memory._l4_global)} ä¸ªfacts")

        # æ‰§è¡Œå‹ç¼©
        await memory._compress_l4()

        print(f"   å‹ç¼©å: {len(memory._l4_global)} ä¸ªfacts")
        print(f"   å‹ç¼©ç‡: {(1 - len(memory._l4_global) / len(similar_facts)) * 100:.1f}%")

        # æ˜¾ç¤ºå‹ç¼©åçš„facts
        print("\nğŸ“‹ å‹ç¼©åçš„facts:")
        for i, fact in enumerate(memory._l4_global):
            content_preview = str(fact.content)[:80]
            metadata = fact.metadata or {}
            compressed_from = metadata.get("compressed_from", 0)

            if compressed_from > 0:
                print(f"   {i+1}. [å‹ç¼©è‡ª{compressed_from}ä¸ª] {content_preview}...")
            else:
                print(f"   {i+1}. [åŸå§‹] {content_preview}...")

    print("\nâœ… é˜¶æ®µ2æµ‹è¯•å®Œæˆ")


# è¿è¡Œæµ‹è¯•
asyncio.run(test_with_verbose_output())
print("\nâœ… åŸºç¡€æµ‹è¯•å®Œæˆ")

print("\n" + "=" * 80)
asyncio.run(test_stage1_projection())
print("\nâœ… é˜¶æ®µ1æµ‹è¯•å®Œæˆ")

print("\n" + "=" * 80)
asyncio.run(test_stage2_l4_compression())
print("\nâœ… é˜¶æ®µ2æµ‹è¯•å®Œæˆ")

