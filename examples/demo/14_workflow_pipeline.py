"""
14_workflow_pipeline.py - å·¥ä½œæµç®¡é“

æ¼”ç¤ºï¼š
- é¡ºåºæ‰§è¡Œçš„å·¥ä½œæµç®¡é“
- æ­¥éª¤é—´æ•°æ®ä¼ é€’
- EventBus äº‹ä»¶ç›‘å¬ï¼ˆæ˜¾ç¤ºå†…éƒ¨å¾ªç¯ï¼‰
- ResultSynthesizer ç»“æœåˆæˆ
"""

import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv
from loom.agent import Agent
from loom.events import EventBus
from loom.providers.llm import OpenAIProvider
from loom.config.llm import LLMConfig
from loom.fractal import ResultSynthesizer
from loom.protocol import Task

# åŠ è½½ .env æ–‡ä»¶
load_dotenv(Path(__file__).parent.parent.parent / ".env")


class WorkflowPipeline:
    """ç®€å•çš„å·¥ä½œæµç®¡é“"""

    def __init__(self, llm, event_bus: EventBus):
        self.llm = llm
        self.event_bus = event_bus
        self.steps = []
        self.results = []

    def add_step(self, name: str, system_prompt: str):
        """æ·»åŠ ç®¡é“æ­¥éª¤"""
        self.steps.append({"name": name, "system_prompt": system_prompt})
        return self

    async def execute(self, initial_input: str) -> list[dict]:
        """æ‰§è¡Œç®¡é“"""
        current_input = initial_input
        self.results = []

        for i, step in enumerate(self.steps):
            print(f"\n[Step {i+1}] {step['name']}...")

            agent = Agent.create(
                llm=self.llm,
                node_id=f"pipeline-step-{i}",
                system_prompt=step["system_prompt"],
                event_bus=self.event_bus,
                max_iterations=3,
            )

            result = await agent.run(current_input)
            self.results.append({
                "step": step["name"],
                "result": result,
                "success": True,
            })

            # ä¸‹ä¸€æ­¥çš„è¾“å…¥æ˜¯å½“å‰æ­¥éª¤çš„è¾“å‡º
            current_input = result
            print(f"    [å®Œæˆ] {result[:60]}...")

        return self.results


async def main():
    # 1. åˆ›å»º LLM Provider
    api_key = os.environ.get("OPENAI_API_KEY")
    base_url = os.environ.get("OPENAI_BASE_URL")
    model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
    if not api_key:
        print("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return

    config = LLMConfig(provider="openai", model=model, api_key=api_key, base_url=base_url)
    llm = OpenAIProvider(config)

    # 2. åˆ›å»º EventBus å¹¶æ³¨å†Œäº‹ä»¶å¤„ç†å™¨ï¼ˆæ˜¾ç¤ºå†…éƒ¨å¾ªç¯ï¼‰
    event_bus = EventBus()

    async def on_thinking(task: Task) -> Task:
        """ç›‘å¬æ€è€ƒè¿‡ç¨‹"""
        content = task.parameters.get("content", "")[:80]
        node_id = task.parameters.get("node_id", "")
        print(f"    ğŸ’­ [{node_id}] æ€è€ƒ: {content}...")
        return task

    async def on_tool_call(task: Task) -> Task:
        """ç›‘å¬å·¥å…·è°ƒç”¨"""
        tool_name = task.parameters.get("tool_name", "")
        tool_args = task.parameters.get("tool_args", {})
        node_id = task.parameters.get("node_id", "")
        print(f"    ğŸ”§ [{node_id}] è°ƒç”¨å·¥å…·: {tool_name}({tool_args})")
        return task

    async def on_tool_result(task: Task) -> Task:
        """ç›‘å¬å·¥å…·ç»“æœ"""
        tool_name = task.parameters.get("tool_name", "")
        result = str(task.parameters.get("result", ""))[:60]
        node_id = task.parameters.get("node_id", "")
        print(f"    âœ… [{node_id}] å·¥å…·ç»“æœ: {tool_name} -> {result}...")
        return task

    # æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    event_bus.register_handler("node.thinking", on_thinking)
    event_bus.register_handler("node.tool_call", on_tool_call)
    event_bus.register_handler("node.tool_result", on_tool_result)

    print("=== å·¥ä½œæµç®¡é“æ¼”ç¤º ===")
    print("åœºæ™¯ï¼šå†…å®¹åˆ›ä½œç®¡é“ï¼ˆæ„æ€ -> å¤§çº² -> æ‘˜è¦ï¼‰\n")

    # 3. åˆ›å»ºå·¥ä½œæµç®¡é“
    pipeline = WorkflowPipeline(llm, event_bus)
    pipeline.add_step(
        name="æ„æ€",
        system_prompt="ä½ æ˜¯åˆ›æ„ä¸“å®¶ã€‚æ ¹æ®ä¸»é¢˜ç”Ÿæˆ3ä¸ªåˆ›æ„ç‚¹å­ï¼Œæ¯ä¸ªä¸€å¥è¯ã€‚"
    ).add_step(
        name="å¤§çº²",
        system_prompt="ä½ æ˜¯å†…å®¹è§„åˆ’å¸ˆã€‚æ ¹æ®åˆ›æ„ç‚¹å­ï¼Œé€‰æ‹©æœ€å¥½çš„ä¸€ä¸ªï¼Œç”Ÿæˆç®€çŸ­å¤§çº²ï¼ˆ3ä¸ªè¦ç‚¹ï¼‰ã€‚"
    ).add_step(
        name="æ‘˜è¦",
        system_prompt="ä½ æ˜¯æ–‡æ¡ˆä¸“å®¶ã€‚æ ¹æ®å¤§çº²ï¼Œå†™ä¸€æ®µ50å­—ä»¥å†…çš„ç²¾ç‚¼æ‘˜è¦ã€‚"
    )

    # 4. æ‰§è¡Œç®¡é“
    initial_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨"
    print(f"åˆå§‹è¾“å…¥: {initial_topic}")

    results = await pipeline.execute(initial_topic)

    # 5. ä½¿ç”¨ ResultSynthesizer åˆæˆç»“æœ
    print("\n--- ç»“æœåˆæˆ ---")
    synthesizer = ResultSynthesizer()

    # ç»“æ„åŒ–åˆæˆ
    structured = await synthesizer.synthesize(
        task=f"å…³äº'{initial_topic}'çš„å†…å®¹åˆ›ä½œ",
        subtask_results=results,
        strategy="structured",
    )
    print(f"\nç»“æ„åŒ–è¾“å‡º:\n{structured}")

    # LLM æ™ºèƒ½åˆæˆ
    print("\n--- LLM æ™ºèƒ½åˆæˆ ---")
    llm_synthesis = await synthesizer.synthesize(
        task=f"å…³äº'{initial_topic}'çš„å†…å®¹åˆ›ä½œ",
        subtask_results=results,
        strategy="llm",
        provider=llm,
    )
    print(f"\næ™ºèƒ½åˆæˆç»“æœ:\n{llm_synthesis}")


if __name__ == "__main__":
    asyncio.run(main())
