"""
å‘é‡æ•°æ®åº“å¿«é€Ÿé…ç½®ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•å¿«é€Ÿè¿æ¥å’Œåˆ‡æ¢ä¸åŒçš„å‘é‡æ•°æ®åº“ï¼š
- Pinecone (äº‘æœåŠ¡)
- Qdrant (æœ¬åœ°/äº‘)
- Milvus (æœ¬åœ°/äº‘)
- ChromaDB (æœ¬åœ°/è¿œç¨‹)

ä»¥åŠä¸åŒçš„ Embedding æœåŠ¡ï¼š
- OpenAI
- Sentence Transformers (æœ¬åœ°)
"""

import asyncio
from loom.components.agent import Agent
from loom.builtin.retriever.vector_store import VectorStoreRetriever
from loom.interfaces.retriever import Document

# å‡è®¾å·²æœ‰ LLM
from loom.llms.openai_llm import OpenAILLM


# ============================================================
# ç¤ºä¾‹ 1: Pinecone + OpenAI Embedding
# ============================================================
async def example_pinecone():
    """Pinecone äº‘æœåŠ¡ + OpenAI Embedding"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: Pinecone + OpenAI Embedding")
    print("=" * 60 + "\n")

    from loom.builtin.retriever.pinecone_store import PineconeVectorStore
    from loom.builtin.embeddings import OpenAIEmbedding
    from loom.builtin.retriever.vector_store_config import PineconeConfig

    # 1. é…ç½® Pinecone
    pinecone_config = PineconeConfig.create(
        api_key="your-pinecone-api-key",
        environment="us-west1-gcp",
        index_name="loom-docs",
        dimension=1536
    )

    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = PineconeVectorStore(pinecone_config)
    await vector_store.initialize()

    # 3. é…ç½® Embedding
    embedding = OpenAIEmbedding(
        api_key="your-openai-api-key",
        model="text-embedding-3-small"
    )

    # 4. åˆ›å»ºæ£€ç´¢å™¨
    retriever = VectorStoreRetriever(
        vector_store=vector_store,
        embedding=embedding
    )

    # 5. æ·»åŠ æ–‡æ¡£
    docs = [
        Document(
            content="Loom æ˜¯ä¸€ä¸ªè½»é‡çº§ AI Agent æ¡†æ¶",
            metadata={"source": "intro.md", "category": "framework"}
        ),
        Document(
            content="Loom æ”¯æŒ RAGã€å·¥å…·ç³»ç»Ÿå’Œè®°å¿†ç®¡ç†",
            metadata={"source": "features.md", "category": "features"}
        ),
    ]
    await retriever.add_documents(docs)

    # 6. æ£€ç´¢
    results = await retriever.retrieve("Loom æ¡†æ¶", top_k=2)
    print(f"âœ… Pinecone æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\n")

    for i, doc in enumerate(results, 1):
        print(f"  [{i}] {doc.content[:50]}... (score: {doc.score:.2f})")

    await vector_store.close()
    print()


# ============================================================
# ç¤ºä¾‹ 2: Qdrant (æœ¬åœ°) + Sentence Transformers
# ============================================================
async def example_qdrant_local():
    """Qdrant æœ¬åœ°éƒ¨ç½² + Sentence Transformers æœ¬åœ°æ¨¡å‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 2: Qdrant (æœ¬åœ°) + Sentence Transformers")
    print("=" * 60 + "\n")

    from loom.builtin.retriever.qdrant_store import QdrantVectorStore
    from loom.builtin.embeddings import SentenceTransformersEmbedding
    from loom.builtin.retriever.vector_store_config import QdrantConfig

    # 1. é…ç½® Qdrantï¼ˆæœ¬åœ° Dockerï¼‰
    qdrant_config = QdrantConfig.create(
        host="localhost",
        port=6333,
        collection_name="loom_docs",
        dimension=384  # MiniLM æ¨¡å‹ç»´åº¦
    )

    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = QdrantVectorStore(qdrant_config)
    await vector_store.initialize()

    # 3. é…ç½® Embeddingï¼ˆå®Œå…¨æœ¬åœ°ï¼Œæ— éœ€ APIï¼‰
    embedding = SentenceTransformersEmbedding(
        model_name="all-MiniLM-L6-v2",  # è½»é‡çº§å¿«é€Ÿæ¨¡å‹
        device="cpu"  # æˆ– "cuda"/"mps"
    )

    # 4. åˆ›å»ºæ£€ç´¢å™¨
    retriever = VectorStoreRetriever(
        vector_store=vector_store,
        embedding=embedding
    )

    # 5. æ·»åŠ æ–‡æ¡£
    docs = [
        Document(
            content="Loom provides a three-layer RAG architecture",
            metadata={"source": "rag.md", "language": "en"}
        ),
        Document(
            content="RAG includes ContextRetriever, DocumentSearchTool, and RAGPattern",
            metadata={"source": "rag_layers.md", "language": "en"}
        ),
    ]
    await retriever.add_documents(docs)

    # 6. æ£€ç´¢
    results = await retriever.retrieve("RAG architecture", top_k=2)
    print(f"âœ… Qdrant æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\n")

    for i, doc in enumerate(results, 1):
        print(f"  [{i}] {doc.content[:50]}... (score: {doc.score:.2f})")

    await vector_store.close()
    print()


# ============================================================
# ç¤ºä¾‹ 3: Milvus (æœ¬åœ°) + OpenAI Embedding
# ============================================================
async def example_milvus():
    """Milvus æœ¬åœ°éƒ¨ç½² + OpenAI Embedding"""
    print("=" * 60)
    print("ç¤ºä¾‹ 3: Milvus (æœ¬åœ°) + OpenAI Embedding")
    print("=" * 60 + "\n")

    from loom.builtin.retriever.milvus_store import MilvusVectorStore
    from loom.builtin.embeddings import OpenAIEmbedding
    from loom.builtin.retriever.vector_store_config import MilvusConfig

    # 1. é…ç½® Milvus
    milvus_config = MilvusConfig.create(
        host="localhost",
        port=19530,
        collection_name="loom_docs",
        dimension=1536,
        index_type="IVF_FLAT"  # æˆ– "HNSW" æ›´å¿«ä½†å å†…å­˜
    )

    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = MilvusVectorStore(milvus_config)
    await vector_store.initialize()

    # 3. é…ç½® Embedding
    embedding = OpenAIEmbedding(
        api_key="your-openai-api-key",
        model="text-embedding-3-small"
    )

    # 4. åˆ›å»ºæ£€ç´¢å™¨
    retriever = VectorStoreRetriever(
        vector_store=vector_store,
        embedding=embedding
    )

    # 5. æ·»åŠ å¤§é‡æ–‡æ¡£ï¼ˆMilvus é€‚åˆæµ·é‡æ•°æ®ï¼‰
    docs = [
        Document(
            content=f"Loom document {i}: Loom is a framework for building AI agents",
            metadata={"doc_id": i, "category": "docs"}
        )
        for i in range(100)  # æ¨¡æ‹Ÿå¤§é‡æ–‡æ¡£
    ]
    await retriever.add_documents(docs)

    # 6. æ£€ç´¢
    results = await retriever.retrieve("Loom framework", top_k=5)
    print(f"âœ… Milvus æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\n")

    for i, doc in enumerate(results, 1):
        print(f"  [{i}] {doc.content[:40]}... (score: {doc.score:.2f})")

    await vector_store.close()
    print()


# ============================================================
# ç¤ºä¾‹ 4: ChromaDB (æœ¬åœ°æŒä¹…åŒ–) + Sentence Transformers
# ============================================================
async def example_chroma_local():
    """ChromaDB æœ¬åœ°æŒä¹…åŒ– + Sentence Transformersï¼ˆé€‚åˆåŸå‹å¼€å‘ï¼‰"""
    print("=" * 60)
    print("ç¤ºä¾‹ 4: ChromaDB (æœ¬åœ°) + Sentence Transformers")
    print("=" * 60 + "\n")

    from loom.builtin.retriever.chroma_store import ChromaVectorStore
    from loom.builtin.embeddings import SentenceTransformersEmbedding
    from loom.builtin.retriever.vector_store_config import ChromaConfig

    # 1. é…ç½® ChromaDBï¼ˆæœ¬åœ°æŒä¹…åŒ–ï¼‰
    chroma_config = ChromaConfig.create_local(
        persist_directory="./chroma_db",
        collection_name="loom_docs",
        dimension=384
    )

    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = ChromaVectorStore(chroma_config)
    await vector_store.initialize()

    # 3. é…ç½® Embedding
    embedding = SentenceTransformersEmbedding(
        model_name="all-MiniLM-L6-v2"
    )

    # 4. åˆ›å»ºæ£€ç´¢å™¨
    retriever = VectorStoreRetriever(
        vector_store=vector_store,
        embedding=embedding
    )

    # 5. æ·»åŠ æ–‡æ¡£
    docs = [
        Document(
            content="Loom supports multiple vector databases: Pinecone, Qdrant, Milvus, ChromaDB",
            metadata={"source": "vector_stores.md"}
        ),
        Document(
            content="ChromaDB is great for rapid prototyping with local persistence",
            metadata={"source": "chroma_intro.md"}
        ),
    ]
    await retriever.add_documents(docs)

    # 6. æ£€ç´¢
    results = await retriever.retrieve("vector databases", top_k=2)
    print(f"âœ… ChromaDB æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\n")

    for i, doc in enumerate(results, 1):
        print(f"  [{i}] {doc.content[:50]}... (score: {doc.score:.2f})")

    await vector_store.close()
    print()


# ============================================================
# ç¤ºä¾‹ 5: å¿«é€Ÿåˆ‡æ¢å‘é‡æ•°æ®åº“ï¼ˆé…ç½®åŒ–ï¼‰
# ============================================================
async def example_config_based_switch():
    """åŸºäºé…ç½®æ–‡ä»¶å¿«é€Ÿåˆ‡æ¢å‘é‡æ•°æ®åº“"""
    print("=" * 60)
    print("ç¤ºä¾‹ 5: é…ç½®åŒ–åˆ‡æ¢å‘é‡æ•°æ®åº“")
    print("=" * 60 + "\n")

    # é…ç½®å­—å…¸ï¼ˆå¯ä»¥ä» YAML/JSON åŠ è½½ï¼‰
    config = {
        "vector_store": {
            "type": "qdrant",  # å¯åˆ‡æ¢ä¸º: pinecone, milvus, chroma
            "host": "localhost",
            "port": 6333,
            "collection_name": "loom_docs",
            "dimension": 384
        },
        "embedding": {
            "type": "sentence_transformers",  # å¯åˆ‡æ¢ä¸º: openai
            "model_name": "all-MiniLM-L6-v2"
        }
    }

    # æ ¹æ®é…ç½®åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = create_vector_store(config["vector_store"])
    await vector_store.initialize()

    # æ ¹æ®é…ç½®åˆ›å»º Embedding
    embedding = create_embedding(config["embedding"])

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = VectorStoreRetriever(
        vector_store=vector_store,
        embedding=embedding
    )

    print(f"âœ… ä½¿ç”¨ {config['vector_store']['type']} + {config['embedding']['type']}\n")

    await vector_store.close()


def create_vector_store(config: dict):
    """æ ¹æ®é…ç½®åˆ›å»ºå‘é‡å­˜å‚¨"""
    store_type = config["type"]

    if store_type == "pinecone":
        from loom.builtin.retriever.pinecone_store import PineconeVectorStore
        return PineconeVectorStore(config)
    elif store_type == "qdrant":
        from loom.builtin.retriever.qdrant_store import QdrantVectorStore
        return QdrantVectorStore(config)
    elif store_type == "milvus":
        from loom.builtin.retriever.milvus_store import MilvusVectorStore
        return MilvusVectorStore(config)
    elif store_type == "chroma":
        from loom.builtin.retriever.chroma_store import ChromaVectorStore
        return ChromaVectorStore(config)
    else:
        raise ValueError(f"Unknown vector store type: {store_type}")


def create_embedding(config: dict):
    """æ ¹æ®é…ç½®åˆ›å»º Embedding"""
    embedding_type = config["type"]

    if embedding_type == "openai":
        from loom.builtin.embeddings import OpenAIEmbedding
        return OpenAIEmbedding(
            api_key=config["api_key"],
            model=config.get("model_name", "text-embedding-3-small")
        )
    elif embedding_type == "sentence_transformers":
        from loom.builtin.embeddings import SentenceTransformersEmbedding
        return SentenceTransformersEmbedding(
            model_name=config.get("model_name", "all-MiniLM-L6-v2")
        )
    else:
        raise ValueError(f"Unknown embedding type: {embedding_type}")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ï¼ˆéœ€è¦å…ˆå¯åŠ¨å¯¹åº”çš„æ•°æ®åº“æœåŠ¡ï¼‰"""
    print("\nğŸš€ Loom å‘é‡æ•°æ®åº“å¿«é€Ÿé…ç½®ç¤ºä¾‹\n")

    # æ³¨æ„ï¼šè¿è¡Œè¿™äº›ç¤ºä¾‹éœ€è¦ï¼š
    # 1. Pinecone: API Key
    # 2. Qdrant: docker run -p 6333:6333 qdrant/qdrant
    # 3. Milvus: docker-compose up (å®˜æ–¹ docker-compose.yml)
    # 4. ChromaDB: æ— éœ€é¢å¤–æœåŠ¡ï¼ˆæœ¬åœ°æŒä¹…åŒ–ï¼‰

    # é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹
    examples = [
        # example_pinecone,          # éœ€è¦ Pinecone API Key
        # example_qdrant_local,      # éœ€è¦ Qdrant æœåŠ¡
        # example_milvus,            # éœ€è¦ Milvus æœåŠ¡
        example_chroma_local,        # æ— éœ€é¢å¤–æœåŠ¡
        example_config_based_switch,
    ]

    for example_func in examples:
        try:
            await example_func()
        except Exception as e:
            print(f"âŒ {example_func.__name__} å¤±è´¥: {e}\n")

    print("âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼\n")
    print("ğŸ’¡ å¿«é€Ÿé€‰æ‹©å»ºè®®:")
    print("  - å¿«é€ŸåŸå‹å¼€å‘: ChromaDB + Sentence Transformersï¼ˆå®Œå…¨æœ¬åœ°ï¼‰")
    print("  - ç”Ÿäº§ç¯å¢ƒï¼ˆäº‘ï¼‰: Pinecone + OpenAI Embedding")
    print("  - ç”Ÿäº§ç¯å¢ƒï¼ˆè‡ªæ‰˜ç®¡ï¼‰: Qdrant/Milvus + OpenAI/Sentence Transformers")
    print("  - æµ·é‡æ•°æ®: Milvus + é«˜æ€§èƒ½ Embedding")
    print()


if __name__ == "__main__":
    asyncio.run(main())
