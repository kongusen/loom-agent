{
  "transformation_patterns": {
    "filtering": {
      "description": "Filter rows based on conditions",
      "examples": [
        {
          "operation": "Simple filter",
          "code": "df[df['age'] > 18]"
        },
        {
          "operation": "Multiple conditions",
          "code": "df[(df['age'] > 18) & (df['status'] == 'active')]"
        },
        {
          "operation": "String matching",
          "code": "df[df['name'].str.contains('John', case=False)]"
        }
      ]
    },
    "aggregation": {
      "description": "Summarize data by groups",
      "examples": [
        {
          "operation": "Group by single column",
          "code": "df.groupby('category')['sales'].sum()"
        },
        {
          "operation": "Multiple aggregations",
          "code": "df.groupby('category').agg({'sales': 'sum', 'quantity': 'mean', 'customer_id': 'count'})"
        },
        {
          "operation": "Custom aggregation",
          "code": "df.groupby('category')['price'].agg(['mean', 'min', 'max', 'std'])"
        }
      ]
    },
    "pivoting": {
      "description": "Reshape data",
      "examples": [
        {
          "operation": "Pivot table",
          "code": "df.pivot_table(values='sales', index='date', columns='category', aggfunc='sum')"
        },
        {
          "operation": "Unpivot (melt)",
          "code": "pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])"
        }
      ]
    },
    "cleaning": {
      "description": "Clean and standardize data",
      "examples": [
        {
          "operation": "Remove duplicates",
          "code": "df.drop_duplicates(subset=['id'], keep='first')"
        },
        {
          "operation": "Handle missing values",
          "code": "df.fillna({'numeric_col': 0, 'text_col': 'unknown'})"
        },
        {
          "operation": "Strip whitespace",
          "code": "df['text_col'] = df['text_col'].str.strip()"
        },
        {
          "operation": "Standardize case",
          "code": "df['text_col'] = df['text_col'].str.lower()"
        }
      ]
    },
    "date_processing": {
      "description": "Work with dates and times",
      "examples": [
        {
          "operation": "Parse dates",
          "code": "df['date'] = pd.to_datetime(df['date_string'])"
        },
        {
          "operation": "Extract date parts",
          "code": "df['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month"
        },
        {
          "operation": "Calculate date differences",
          "code": "df['days_diff'] = (df['end_date'] - df['start_date']).dt.days"
        }
      ]
    }
  },
  "common_pipelines": [
    {
      "name": "Data Cleaning Pipeline",
      "steps": [
        "1. Remove duplicates",
        "2. Handle missing values",
        "3. Standardize text (lowercase, strip)",
        "4. Validate data types",
        "5. Remove outliers"
      ]
    },
    {
      "name": "ETL Pipeline",
      "steps": [
        "1. Extract: Read from source",
        "2. Transform: Clean, filter, aggregate",
        "3. Load: Write to destination",
        "4. Validate: Check data quality"
      ]
    },
    {
      "name": "Reporting Pipeline",
      "steps": [
        "1. Load data",
        "2. Filter relevant period",
        "3. Aggregate by dimensions",
        "4. Calculate KPIs",
        "5. Format for presentation"
      ]
    }
  ],
  "performance_tips": [
    "Use vectorized operations instead of loops",
    "Specify dtypes when reading to reduce memory",
    "Use categorical dtype for low-cardinality columns",
    "Process data in chunks for large files",
    "Use query() method for complex filters (faster)",
    "Avoid chained indexing (use .loc[])",
    "Use inplace=True cautiously (not always faster)"
  ]
}
