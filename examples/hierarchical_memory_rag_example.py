"""
HierarchicalMemory + RAG é›†æˆç¤ºä¾‹

æ¼”ç¤ºåˆ†å±‚è®°å¿†ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. 4å±‚è®°å¿†ç®¡ç†ï¼ˆEphemeral/Working/Session/Long-termï¼‰
2. è¯­ä¹‰æ£€ç´¢ï¼ˆRAGï¼‰
3. è‡ªåŠ¨æ™‹å‡æœºåˆ¶
4. å·¥å…·è®°å¿†ç®¡ç†
5. ä¸ ContextAssembler é›†æˆ

Requirements:
    pip install loom-agent openai
    export OPENAI_API_KEY=your_api_key_here
"""

import asyncio
import os
from loom.builtin.memory import HierarchicalMemory, MemoryEntry
from loom.builtin.embeddings import OpenAIEmbedding
from loom.builtin.vector_store import InMemoryVectorStore
from loom.core.message import Message
from loom.core.context_assembler import EnhancedContextManager, create_enhanced_context_manager


# ============================================================================
# Example 1: åŸºç¡€ç”¨æ³• - é›¶é…ç½®å¯åŠ¨
# ============================================================================

async def example_1_basic_usage():
    """
    æœ€ç®€å•çš„ç”¨æ³•ï¼šæ— å‘é‡åŒ–èƒ½åŠ›ï¼Œä½¿ç”¨å…³é”®è¯æ£€ç´¢é™çº§
    """
    print("=" * 80)
    print("Example 1: åŸºç¡€ç”¨æ³• - é›¶é…ç½®å¯åŠ¨ï¼ˆå…³é”®è¯æ£€ç´¢ï¼‰")
    print("=" * 80)

    # åˆ›å»ºä¸å¸¦å‘é‡åŒ–çš„ HierarchicalMemoryï¼ˆå…³é”®è¯æ£€ç´¢é™çº§ï¼‰
    memory = HierarchicalMemory(
        enable_persistence=False,
        auto_promote=True,
        working_memory_size=5,
    )

    # æ·»åŠ ä¸€äº›å¯¹è¯æ¶ˆæ¯åˆ° Session Memory
    messages = [
        Message(role="user", content="æˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ"),
        Message(role="assistant", content="ä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"),
        Message(role="user", content="æˆ‘å–œæ¬¢Pythonå’Œæœºå™¨å­¦ä¹ "),
        Message(role="assistant", content="å¤ªå¥½äº†ï¼Pythonåœ¨MLé¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚"),
    ]

    for msg in messages:
        async for event in memory.add_message_stream(msg):
            if event.data and "message" in event.data:
                print(f"  [Event] {event.type.value}: {event.data['message']}")

    # æŸ¥è¯¢è®°å¿†ï¼ˆå…³é”®è¯æ£€ç´¢ï¼‰
    print("\næŸ¥è¯¢: 'å¼ ä¸‰æ˜¯è°ï¼Ÿ'")
    result = await memory.retrieve(query="å¼ ä¸‰æ˜¯è°ï¼Ÿ", top_k=3)
    print(f"æ£€ç´¢ç»“æœ:\n{result}")

    # æŸ¥çœ‹ä¸åŒå±‚çº§çš„è®°å¿†
    print("\n[Session Memory]")
    session_msgs = await memory.get_by_tier("session")
    for msg in session_msgs[-2:]:
        print(f"  - {msg.role}: {msg.content[:50]}...")

    print("\n[Working Memory]")
    working_msgs = await memory.get_by_tier("working")
    print(f"  å…± {len(working_msgs)} æ¡é‡è¦è®°å¿†")


# ============================================================================
# Example 2: RAG è¯­ä¹‰æ£€ç´¢ - å‘é‡åŒ–è®°å¿†
# ============================================================================

async def example_2_rag_semantic_search():
    """
    ä½¿ç”¨ OpenAI Embedding è¿›è¡Œè¯­ä¹‰æ£€ç´¢
    """
    print("\n" + "=" * 80)
    print("Example 2: RAG è¯­ä¹‰æ£€ç´¢ - å‘é‡åŒ–è®°å¿†")
    print("=" * 80)

    # æ£€æŸ¥ API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æ­¤ç¤ºä¾‹")
        return

    # åˆ›å»ºå¸¦å‘é‡åŒ–èƒ½åŠ›çš„ HierarchicalMemory
    embedding = OpenAIEmbedding(model="text-embedding-3-small")
    vector_store = InMemoryVectorStore(dimension=1536, use_faiss=True)

    memory = HierarchicalMemory(
        embedding=embedding,
        vector_store=vector_store,
        enable_persistence=False,
        auto_promote=True,
        working_memory_size=3,  # è¾ƒå°çš„å®¹é‡ï¼Œä¾¿äºæ¼”ç¤ºæ™‹å‡
    )

    await vector_store.initialize()

    # æ·»åŠ å¯¹è¯ï¼ˆä¼šè‡ªåŠ¨æå–åˆ° Working Memoryï¼‰
    conversations = [
        ("user", "æˆ‘æœ€è¿‘åœ¨å­¦ä¹  Rust ç¼–ç¨‹è¯­è¨€ï¼Œè§‰å¾—å®ƒçš„æ‰€æœ‰æƒç³»ç»Ÿå¾ˆæœ‰æ„æ€"),
        ("assistant", "Rust çš„æ‰€æœ‰æƒç³»ç»Ÿç¡®å®æ˜¯å…¶æ ¸å¿ƒç‰¹æ€§ï¼Œèƒ½åœ¨ç¼–è¯‘æœŸä¿è¯å†…å­˜å®‰å…¨ã€‚"),
        ("user", "å¯¹ï¼Œè€Œä¸”æ²¡æœ‰åƒåœ¾å›æ”¶å™¨ï¼Œæ€§èƒ½å¾ˆå¥½"),
        ("assistant", "æ˜¯çš„ï¼ŒRust åœ¨ç³»ç»Ÿç¼–ç¨‹é¢†åŸŸå¾ˆå—æ¬¢è¿ï¼Œæ¯”å¦‚ç”¨äºæ“ä½œç³»ç»Ÿã€æµè§ˆå™¨å¼•æ“ç­‰ã€‚"),
        ("user", "æˆ‘ä¹Ÿåœ¨å…³æ³¨ WebAssemblyï¼ŒRust å¯¹ WASM æ”¯æŒå¾ˆæ£’"),
        ("assistant", "æ²¡é”™ï¼ŒRust æ˜¯ WebAssembly çš„é¦–é€‰è¯­è¨€ä¹‹ä¸€ï¼Œç”Ÿæ€å¾ˆä¸°å¯Œã€‚"),
    ]

    print("\næ·»åŠ å¯¹è¯åˆ°è®°å¿†...")
    for role, content in conversations:
        msg = Message(role=role, content=content)
        async for event in memory.add_message_stream(msg):
            pass  # é™é»˜å¤„ç†äº‹ä»¶

    # æ‰‹åŠ¨æ·»åŠ ä¸€äº›é•¿æœŸè®°å¿†ï¼ˆç”¨æˆ·ç”»åƒï¼‰
    print("\næ·»åŠ ç”¨æˆ·ç”»åƒåˆ°é•¿æœŸè®°å¿†...")
    await memory.add_to_longterm(
        content="ç”¨æˆ·æ˜¯ä¸€åå¯¹ç³»ç»Ÿç¼–ç¨‹å’Œæ€§èƒ½ä¼˜åŒ–æ„Ÿå…´è¶£çš„å¼€å‘è€…ï¼Œæ­£åœ¨å­¦ä¹  Rust è¯­è¨€ã€‚",
        metadata={"category": "user_profile", "importance": "high"}
    )

    await memory.add_to_longterm(
        content="ç”¨æˆ·å…³æ³¨å‰æ²¿æŠ€æœ¯ï¼ŒåŒ…æ‹¬ WebAssemblyã€æµè§ˆå™¨æŠ€æœ¯ç­‰ã€‚",
        metadata={"category": "user_interests"}
    )

    # è¯­ä¹‰æ£€ç´¢ï¼ˆå‘é‡æœç´¢ï¼‰
    print("\n" + "-" * 80)
    print("è¯­ä¹‰æ£€ç´¢æµ‹è¯•ï¼š")

    queries = [
        "ç”¨æˆ·å–œæ¬¢ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ",
        "ç”¨æˆ·å¯¹æ€§èƒ½ä¼˜åŒ–æœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿ",
        "ç”¨æˆ·çš„æŠ€æœ¯èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ",
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: '{query}'")
        result = await memory.retrieve(query=query, top_k=2, tier="longterm")
        print(f"æ£€ç´¢ç»“æœ:\n{result}")

    # æŸ¥çœ‹ä¸åŒå±‚çº§çš„è®°å¿†ç»Ÿè®¡
    print("\n" + "-" * 80)
    print("è®°å¿†å±‚çº§ç»Ÿè®¡:")
    for tier in ["ephemeral", "working", "session", "longterm"]:
        msgs = await memory.get_by_tier(tier)
        print(f"  [{tier.upper()}] {len(msgs)} æ¡è®°å¿†")


# ============================================================================
# Example 3: å·¥å…·è®°å¿†ï¼ˆEphemeral Memoryï¼‰
# ============================================================================

async def example_3_tool_memory():
    """
    æ¼”ç¤ºå·¥å…·è°ƒç”¨çš„ä¸´æ—¶è®°å¿†ç®¡ç†
    """
    print("\n" + "=" * 80)
    print("Example 3: å·¥å…·è®°å¿†ï¼ˆEphemeral Memoryï¼‰- ç”¨å®Œå°±ä¸¢")
    print("=" * 80)

    memory = HierarchicalMemory(enable_persistence=False)

    # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨æµç¨‹
    tool_id = "call_abc123"
    tool_name = "search_database"

    # 1. å·¥å…·è°ƒç”¨å¼€å§‹ - æ·»åŠ ä¸´æ—¶è®°å¿†
    print(f"\n[æ­¥éª¤ 1] å·¥å…·è°ƒç”¨å¼€å§‹: {tool_name}")
    await memory.add_ephemeral(
        key=f"tool_{tool_id}",
        content=f"Calling {tool_name} with args: {{'query': 'user profile'}}",
        metadata={"tool_name": tool_name, "status": "in_progress"}
    )

    # æŸ¥çœ‹ Ephemeral Memory
    ephemeral_msgs = await memory.get_by_tier("ephemeral")
    print(f"  ä¸´æ—¶è®°å¿†: {len(ephemeral_msgs)} æ¡")
    for msg in ephemeral_msgs:
        print(f"    - {msg.content}")

    # 2. æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
    print(f"\n[æ­¥éª¤ 2] å·¥å…·æ‰§è¡Œä¸­...")
    await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ

    # 3. å·¥å…·è°ƒç”¨å®Œæˆ - ä¿å­˜æœ€ç»ˆç»“æœåˆ° Session Memory
    print(f"\n[æ­¥éª¤ 3] å·¥å…·è°ƒç”¨å®Œæˆï¼Œä¿å­˜ç»“æœåˆ° Session Memory")
    result_msg = Message(
        role="tool",
        content="Found user: Zhang San, Engineer, Interests: Python, ML",
        name=tool_name,
    )
    async for event in memory.add_message_stream(result_msg):
        pass

    # 4. æ¸…ç†ä¸´æ—¶è®°å¿†
    print(f"\n[æ­¥éª¤ 4] æ¸…ç†ä¸´æ—¶è®°å¿†")
    await memory.clear_ephemeral(key=f"tool_{tool_id}")

    ephemeral_msgs_after = await memory.get_by_tier("ephemeral")
    session_msgs = await memory.get_by_tier("session")

    print(f"  ä¸´æ—¶è®°å¿†: {len(ephemeral_msgs_after)} æ¡ï¼ˆå·²æ¸…ç©ºï¼‰")
    print(f"  ä¼šè¯è®°å¿†: {len(session_msgs)} æ¡ï¼ˆä¿ç•™æœ€ç»ˆç»“æœï¼‰")

    # éªŒè¯åªæœ‰æœ€ç»ˆç»“æœè¢«ä¿ç•™
    print("\næœ€ç»ˆä¿ç•™çš„è®°å¿†:")
    for msg in session_msgs:
        print(f"  - [{msg.role}] {msg.content}")


# ============================================================================
# Example 4: è‡ªåŠ¨æ™‹å‡æœºåˆ¶
# ============================================================================

async def example_4_auto_promotion():
    """
    æ¼”ç¤º Working Memory â†’ Long-term Memory è‡ªåŠ¨æ™‹å‡
    """
    print("\n" + "=" * 80)
    print("Example 4: è‡ªåŠ¨æ™‹å‡æœºåˆ¶ (Working â†’ Long-term)")
    print("=" * 80)

    # åˆ›å»ºå®¹é‡å¾ˆå°çš„ Working Memoryï¼Œä¾¿äºæ¼”ç¤ºæ™‹å‡
    if os.getenv("OPENAI_API_KEY"):
        embedding = OpenAIEmbedding(model="text-embedding-3-small")
        vector_store = InMemoryVectorStore(dimension=1536)
        await vector_store.initialize()
    else:
        print("âš ï¸  æœªè®¾ç½® OPENAI_API_KEYï¼Œä½¿ç”¨æ— å‘é‡åŒ–æ¨¡å¼")
        embedding = None
        vector_store = None

    memory = HierarchicalMemory(
        embedding=embedding,
        vector_store=vector_store,
        enable_persistence=False,
        auto_promote=True,
        working_memory_size=2,  # åªä¿ç•™æœ€è¿‘ 2 æ¡
    )

    # æ·»åŠ å¤šæ¡æ¶ˆæ¯ï¼Œè§¦å‘è‡ªåŠ¨æ™‹å‡
    messages = [
        "ç”¨æˆ·æåˆ°ä»–åœ¨ä½¿ç”¨ Docker è¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²",
        "ç”¨æˆ·è¯´ä»–çš„å›¢é˜Ÿåœ¨ç”¨ Kubernetes ç¼–æ’",
        "ç”¨æˆ·è¯¢é—®å…³äºå¾®æœåŠ¡æ¶æ„çš„æœ€ä½³å®è·µ",
        "ç”¨æˆ·è¡¨ç¤ºå¯¹æœåŠ¡ç½‘æ ¼ï¼ˆService Meshï¼‰æ„Ÿå…´è¶£",
        "ç”¨æˆ·æƒ³äº†è§£ Istio çš„å·¥ä½œåŸç†",
    ]

    for i, content in enumerate(messages, 1):
        print(f"\næ·»åŠ ç¬¬ {i} æ¡æ¶ˆæ¯: {content[:40]}...")
        msg = Message(role="user", content=content)
        async for event in memory.add_message_stream(msg):
            pass

        # æŸ¥çœ‹å„å±‚çº§çŠ¶æ€
        working = await memory.get_by_tier("working")
        longterm = await memory.get_by_tier("longterm")

        print(f"  Working Memory: {len(working)} æ¡")
        print(f"  Long-term Memory: {len(longterm)} æ¡")

        if len(longterm) > 0:
            print(f"    â†’ å·²æ™‹å‡åˆ°é•¿æœŸè®°å¿†: {longterm[-1].content[:50]}...")

    # æœ€ç»ˆçŠ¶æ€
    print("\n" + "-" * 80)
    print("æœ€ç»ˆè®°å¿†åˆ†å¸ƒ:")
    print(f"  Working Memory: {len(await memory.get_by_tier('working'))} æ¡ï¼ˆæœ€è¿‘ï¼‰")
    print(f"  Long-term Memory: {len(await memory.get_by_tier('longterm'))} æ¡ï¼ˆå†å²ï¼‰")


# ============================================================================
# Example 5: ä¸ ContextAssembler é›†æˆ
# ============================================================================

async def example_5_context_integration():
    """
    æ¼”ç¤º HierarchicalMemory ä¸ EnhancedContextManager é›†æˆ
    """
    print("\n" + "=" * 80)
    print("Example 5: ä¸ ContextAssembler é›†æˆ")
    print("=" * 80)

    # åˆ›å»ºå¸¦å‘é‡åŒ–çš„ HierarchicalMemory
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æ­¤ç¤ºä¾‹")
        return

    embedding = OpenAIEmbedding(model="text-embedding-3-small")
    vector_store = InMemoryVectorStore(dimension=1536)
    await vector_store.initialize()

    memory = HierarchicalMemory(
        embedding=embedding,
        vector_store=vector_store,
        auto_promote=True,
    )

    # æ·»åŠ ä¸€äº›å†å²å¯¹è¯
    print("\næ„å»ºå†å²å¯¹è¯...")
    history = [
        ("user", "æˆ‘æ˜¯ä¸€å Python å¼€å‘è€…ï¼Œä¸»è¦åšæ•°æ®åˆ†æ"),
        ("assistant", "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Python åœ¨æ•°æ®åˆ†æé¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚"),
        ("user", "æˆ‘å¸¸ç”¨ pandas å’Œ numpy"),
        ("assistant", "è¿™ä¸¤ä¸ªæ˜¯æ•°æ®åˆ†æçš„æ ¸å¿ƒåº“ï¼Œéå¸¸å®ç”¨ã€‚"),
    ]

    for role, content in history:
        msg = Message(role=role, content=content)
        async for event in memory.add_message_stream(msg):
            pass

    # æ·»åŠ ç”¨æˆ·ç”»åƒåˆ°é•¿æœŸè®°å¿†
    await memory.add_to_longterm(
        content="ç”¨æˆ·æ˜¯ Python æ•°æ®åˆ†æå¸ˆï¼Œç†Ÿæ‚‰ pandasã€numpy ç­‰åº“ã€‚",
        metadata={"category": "user_profile"}
    )

    # åˆ›å»º EnhancedContextManagerï¼ˆé›†æˆ Memoryï¼‰
    print("\nåˆ›å»º EnhancedContextManager...")
    context_manager = create_enhanced_context_manager(
        memory=memory,
        max_context_tokens=8000,
        enable_smart_assembly=True,
    )

    # å‡†å¤‡æ–°çš„ç”¨æˆ·æ¶ˆæ¯
    print("\nå‡†å¤‡æ–°æ¶ˆæ¯çš„ä¸Šä¸‹æ–‡...")
    new_message = Message(
        role="user",
        content="æˆ‘æƒ³å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œæœ‰ä»€ä¹ˆæ¨èçš„åº“å—ï¼Ÿ"
    )

    # prepare() ä¼šè‡ªåŠ¨è°ƒç”¨ memory.retrieve() æ£€ç´¢ç›¸å…³é•¿æœŸè®°å¿†
    prepared_message = await context_manager.prepare(new_message)

    # æŸ¥çœ‹ç»„è£…åçš„ä¸Šä¸‹æ–‡ç»“æ„
    print("\nä¸Šä¸‹æ–‡ç»„è£…å®Œæˆï¼")
    print("  åŒ…å«å†…å®¹:")
    print("    - ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰")
    print("    - æ£€ç´¢åˆ°çš„é•¿æœŸè®°å¿†ï¼ˆRAGï¼‰")
    print("    - ä¼šè¯å†å²ï¼ˆSession Memoryï¼‰")
    print("    - å½“å‰ç”¨æˆ·æ¶ˆæ¯")

    print(f"\n  æ€» Token æ•°ä¼°ç®—: ~{len(prepared_message.content.split())} tokens")

    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„è®°å¿†ï¼ˆå¦‚æœæœ‰ï¼‰
    if "retrieved_memory" in prepared_message.content.lower():
        print("\n  âœ“ æˆåŠŸä»é•¿æœŸè®°å¿†æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆRAGï¼‰")


# ============================================================================
# Example 6: å®Œæ•´å·¥ä½œæµ
# ============================================================================

async def example_6_full_workflow():
    """
    å®Œæ•´çš„ Agent å·¥ä½œæµï¼šå¯¹è¯ â†’ å·¥å…·è°ƒç”¨ â†’ è®°å¿†ç®¡ç† â†’ RAG æ£€ç´¢
    """
    print("\n" + "=" * 80)
    print("Example 6: å®Œæ•´å·¥ä½œæµ")
    print("=" * 80)

    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æ­¤ç¤ºä¾‹")
        return

    # åˆå§‹åŒ–ç³»ç»Ÿ
    embedding = OpenAIEmbedding(model="text-embedding-3-small")
    vector_store = InMemoryVectorStore(dimension=1536)
    await vector_store.initialize()

    memory = HierarchicalMemory(
        embedding=embedding,
        vector_store=vector_store,
        auto_promote=True,
        working_memory_size=5,
    )

    print("\n[åœºæ™¯] ç”¨æˆ·ä¸ AI åŠ©æ‰‹çš„å¤šè½®å¯¹è¯ + å·¥å…·è°ƒç”¨")

    # Round 1: ç”¨æˆ·ä»‹ç»è‡ªå·±
    print("\n--- Round 1: ç”¨æˆ·ä»‹ç» ---")
    msg1 = Message(role="user", content="ä½ å¥½ï¼Œæˆ‘æ˜¯ææ˜ï¼Œä¸€åå‰ç«¯å·¥ç¨‹å¸ˆ")
    async for _ in memory.add_message_stream(msg1):
        pass

    msg2 = Message(role="assistant", content="ä½ å¥½ææ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚")
    async for _ in memory.add_message_stream(msg2):
        pass

    # ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°é•¿æœŸè®°å¿†
    await memory.add_to_longterm("ç”¨æˆ·ææ˜æ˜¯ä¸€åå‰ç«¯å·¥ç¨‹å¸ˆ", metadata={"type": "profile"})

    # Round 2: å·¥å…·è°ƒç”¨ï¼ˆæŸ¥è¯¢å¤©æ°”ï¼‰
    print("\n--- Round 2: å·¥å…·è°ƒç”¨ ---")
    msg3 = Message(role="user", content="åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
    async for _ in memory.add_message_stream(msg3):
        pass

    # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨æµç¨‹
    tool_id = "call_weather_001"
    print("  [å·¥å…·] è°ƒç”¨ get_weather æŸ¥è¯¢åŒ—äº¬å¤©æ°”...")

    # æ·»åŠ ä¸´æ—¶è®°å¿†
    await memory.add_ephemeral(
        key=f"tool_{tool_id}",
        content="Calling get_weather(city='Beijing')",
        metadata={"tool": "get_weather", "status": "running"}
    )

    await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿ API è°ƒç”¨

    # ä¿å­˜å·¥å…·ç»“æœåˆ° Session Memory
    tool_result = Message(
        role="tool",
        content="åŒ—äº¬ä»Šå¤©æ™´ï¼Œæ¸©åº¦ 15-25Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
        name="get_weather"
    )
    async for _ in memory.add_message_stream(tool_result):
        pass

    # æ¸…ç†ä¸´æ—¶è®°å¿†
    await memory.clear_ephemeral(key=f"tool_{tool_id}")

    msg4 = Message(role="assistant", content="åŒ—äº¬ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œæ™´å¤©ï¼Œæ¸©åº¦é€‚å®œã€‚")
    async for _ in memory.add_message_stream(msg4):
        pass

    # Round 3: ç›¸å…³æŸ¥è¯¢ï¼ˆè§¦å‘ RAG æ£€ç´¢ï¼‰
    print("\n--- Round 3: RAG æ£€ç´¢ ---")
    query = "è¿™ä¸ªäººæ˜¯åšä»€ä¹ˆå·¥ä½œçš„ï¼Ÿ"
    print(f"  æŸ¥è¯¢: '{query}'")

    result = await memory.retrieve(query=query, top_k=2, tier="longterm")
    print(f"  æ£€ç´¢ç»“æœ: {result[:100]}...")

    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "-" * 80)
    print("æœ€ç»ˆè®°å¿†ç»Ÿè®¡:")
    for tier in ["ephemeral", "working", "session", "longterm"]:
        count = len(await memory.get_by_tier(tier))
        print(f"  [{tier.upper():12}] {count:2} æ¡")

    print("\nâœ“ å®Œæ•´å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸ§ " * 40)
    print("HierarchicalMemory + RAG é›†æˆç¤ºä¾‹")
    print("ğŸ§ " * 40)

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    await example_1_basic_usage()
    await example_2_rag_semantic_search()
    await example_3_tool_memory()
    await example_4_auto_promotion()
    await example_5_context_integration()
    await example_6_full_workflow()

    print("\n" + "=" * 80)
    print("æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 80)
    print("\nå…³é”®è¦ç‚¹æ€»ç»“:")
    print("  1. é›¶é…ç½®ï¼šæ— éœ€å‘é‡åŒ–ä¹Ÿèƒ½ä½¿ç”¨ï¼ˆå…³é”®è¯æ£€ç´¢é™çº§ï¼‰")
    print("  2. RAG é›†æˆï¼šè¯­ä¹‰æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡")
    print("  3. åˆ†å±‚ç®¡ç†ï¼š4 å±‚è®°å¿†è‡ªåŠ¨æµè½¬")
    print("  4. å·¥å…·è®°å¿†ï¼šç”¨å®Œå°±ä¸¢ï¼Œåªä¿ç•™ç»“æœ")
    print("  5. è‡ªåŠ¨æ™‹å‡ï¼šWorking â†’ Long-term æ™ºèƒ½æ™‹å‡")
    print("  6. æ— ç¼é›†æˆï¼šä¸ ContextAssembler å®Œç¾é…åˆ")
    print()


if __name__ == "__main__":
    asyncio.run(main())
