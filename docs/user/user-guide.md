# Loom Agent v4.0.0 ç”¨æˆ·ä½¿ç”¨æŒ‡å—

**ç‰ˆæœ¬**: v4.0.0
**æœ€åæ›´æ–°**: 2025-10-16

---

## ğŸ“– ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
3. [åˆ›å»ºAgent](#åˆ›å»ºagent)
4. [å·¥å…·ç³»ç»Ÿ](#å·¥å…·ç³»ç»Ÿ)
5. [RAGæ¨¡å¼](#ragæ¨¡å¼)
6. [å†…å­˜ç®¡ç†](#å†…å­˜ç®¡ç†)
7. [ç”Ÿäº§ç¯å¢ƒé…ç½®](#ç”Ÿäº§ç¯å¢ƒé…ç½®)
8. [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
9. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# ä»PyPIå®‰è£…
pip install loom-agent

# æˆ–ä»æºç å®‰è£…
git clone https://github.com/your-org/loom-agent.git
cd loom-agent
pip install -e .

# å®‰è£…ç‰¹å®šprovider
pip install "loom-agent[openai]"      # OpenAI
pip install "loom-agent[anthropic]"   # Anthropic Claude
pip install "loom-agent[all]"         # æ‰€æœ‰åŠŸèƒ½
```

### ç¬¬ä¸€ä¸ªAgent

```python
import asyncio
from loom import agent

async def main():
    # æœ€ç®€å•çš„æ–¹å¼ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
    my_agent = agent(
        provider="openai",
        model="gpt-4",
        api_key="your-api-key-here"
    )

    result = await my_agent.ainvoke("ä»‹ç»ä¸€ä¸‹Loom Agentæ¡†æ¶")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### ä½¿ç”¨ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export LOOM_PROVIDER=openai
export OPENAI_API_KEY=sk-...
export LOOM_MODEL=gpt-4

# Pythonä»£ç 
from loom import agent_from_env

async def main():
    my_agent = agent_from_env()  # è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
    result = await my_agent.ainvoke("Hello!")
    print(result)
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Agent (æ™ºèƒ½ä½“)

Agentæ˜¯Loomæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
- æ¥æ”¶ç”¨æˆ·è¾“å…¥
- è°ƒç”¨LLMç”Ÿæˆå“åº”
- æ‰§è¡Œå·¥å…·è°ƒç”¨
- ç®¡ç†å¯¹è¯å†å²
- å¤„ç†é”™è¯¯å’Œé‡è¯•

### 2. LLM (å¤§è¯­è¨€æ¨¡å‹)

LLMæ˜¯Agentçš„"å¤§è„‘"ï¼Œæ”¯æŒå¤šç§æä¾›å•†ï¼š
- OpenAI (GPT-3.5, GPT-4)
- Anthropic (Claude)
- Azure OpenAI
- Google (Gemini)
- Cohere
- Ollama (æœ¬åœ°æ¨¡å‹)

### 3. Tools (å·¥å…·)

å·¥å…·æ˜¯Agentå¯ä»¥è°ƒç”¨çš„å‡½æ•°ï¼Œç”¨äºï¼š
- è¯»å–/å†™å…¥æ–‡ä»¶
- æœç´¢ç½‘ç»œ
- æ‰§è¡Œä»£ç 
- è®¿é—®æ•°æ®åº“
- è°ƒç”¨API

### 4. Memory (å†…å­˜)

å†…å­˜ç³»ç»Ÿç®¡ç†å¯¹è¯å†å²ï¼š
- InMemoryMemory: ä¸´æ—¶å†…å­˜ï¼ˆè¿›ç¨‹å†…ï¼‰
- PersistentMemory: æŒä¹…åŒ–å†…å­˜ï¼ˆè·¨ä¼šè¯ï¼‰

### 5. RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)

RAGæ¨¡å¼é€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºAgentçš„å›ç­”èƒ½åŠ›ã€‚

---

## åˆ›å»ºAgent

### æ–¹æ³•1: ä½¿ç”¨ `loom.agent()` å‡½æ•°

æœ€ç®€å•å’Œæ¨èçš„æ–¹å¼ï¼š

```python
from loom import agent

# æ–¹å¼A: ç›´æ¥æŒ‡å®šproviderå’Œmodel
my_agent = agent(
    provider="openai",
    model="gpt-4",
    api_key="sk-...",
    temperature=0.7,
    max_tokens=2000
)

# æ–¹å¼B: ä½¿ç”¨LLMå®ä¾‹
from loom.builtin.llms import OpenAILLM

llm = OpenAILLM(api_key="sk-...", model="gpt-4")
my_agent = agent(llm=llm)

# æ–¹å¼C: ä½¿ç”¨LLMConfig
from loom import LLMConfig, agent

config = LLMConfig.openai(
    api_key="sk-...",
    model="gpt-4",
    temperature=0.7
)
my_agent = agent(config=config)
```

### æ–¹æ³•2: ä½¿ç”¨ `loom.agent_from_env()`

ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨é…ç½®ï¼š

```python
from loom import agent_from_env

# éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
# LOOM_PROVIDER=openai
# LOOM_MODEL=gpt-4
# OPENAI_API_KEY=sk-...

my_agent = agent_from_env()
```

### æ–¹æ³•3: ä½¿ç”¨ `Agent` ç±»

å®Œå…¨æ§åˆ¶æ‰€æœ‰å‚æ•°ï¼š

```python
from loom import Agent
from loom.builtin.llms import OpenAILLM
from loom.builtin.memory import PersistentMemory

llm = OpenAILLM(api_key="sk-...", model="gpt-4")
memory = PersistentMemory()

my_agent = Agent(
    llm=llm,
    memory=memory,
    max_iterations=50,
    max_context_tokens=16000,
    system_instructions="You are a helpful assistant."
)
```

### Agentå‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `llm` | BaseLLM | å¿…éœ€ | è¯­è¨€æ¨¡å‹å®ä¾‹ |
| `tools` | List[BaseTool] | None | å·¥å…·åˆ—è¡¨ |
| `memory` | BaseMemory | InMemoryMemory | å†…å­˜ç³»ç»Ÿ |
| `compressor` | BaseCompressor | è‡ªåŠ¨ | ä¸Šä¸‹æ–‡å‹ç¼©å™¨ |
| `max_iterations` | int | 50 | æœ€å¤§è¿­ä»£æ¬¡æ•° |
| `max_context_tokens` | int | 16000 | æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•° |
| `system_instructions` | str | None | ç³»ç»Ÿæç¤ºè¯ |
| `callbacks` | List[BaseCallback] | None | å›è°ƒå‡½æ•°åˆ—è¡¨ |
| `safe_mode` | bool | False | å®‰å…¨æ¨¡å¼ï¼ˆéœ€è¦ç¡®è®¤å·¥å…·è°ƒç”¨ï¼‰ |

### è¿è¡ŒAgent

```python
# æ–¹å¼1: ä½¿ç”¨ run() æˆ– ainvoke()
result = await my_agent.run("ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
result = await my_agent.ainvoke("åŒä¸Š")  # LangChainé£æ ¼åˆ«å

# æ–¹å¼2: æµå¼è¾“å‡º
async for event in my_agent.stream("è®²ä¸ªæ•…äº‹"):
    if event.type == "text":
        print(event.content, end="", flush=True)

# æ–¹å¼3: å¸¦å–æ¶ˆä»¤ç‰Œï¼ˆv4.0.0æ–°ç‰¹æ€§ï¼‰
import asyncio

cancel_token = asyncio.Event()

# åœ¨å¦ä¸€ä¸ªä»»åŠ¡ä¸­å¯ä»¥å–æ¶ˆ
# cancel_token.set()

result = await my_agent.run(
    "æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡",
    cancel_token=cancel_token,
    correlation_id="req-123"  # ç”¨äºè¿½è¸ª
)
```

---

## å·¥å…·ç³»ç»Ÿ

### åˆ›å»ºè‡ªå®šä¹‰å·¥å…·

#### æ–¹å¼1: ä½¿ç”¨ `@tool` è£…é¥°å™¨ï¼ˆæ¨èï¼‰

```python
from loom import tool, agent

@tool()
def add(a: int, b: int) -> int:
    """å°†ä¸¤ä¸ªæ•´æ•°ç›¸åŠ """
    return a + b

@tool()
def search_web(query: str, max_results: int = 5) -> str:
    """æœç´¢ç½‘ç»œå¹¶è¿”å›ç»“æœ"""
    # å®ç°æœç´¢é€»è¾‘
    return f"æœç´¢ç»“æœ: {query}"

# åˆ›å»ºå¸¦å·¥å…·çš„Agent
my_agent = agent(
    provider="openai",
    model="gpt-4",
    api_key="sk-...",
    tools=[add(), search_web()]
)

# Agentä¼šè‡ªåŠ¨è°ƒç”¨å·¥å…·
result = await my_agent.run("3åŠ 5ç­‰äºå¤šå°‘ï¼Ÿ")
```

#### æ–¹å¼2: ç»§æ‰¿ `BaseTool` ç±»

```python
from loom.interfaces.tool import BaseTool
from pydantic import BaseModel, Field

class CalculatorArgs(BaseModel):
    expression: str = Field(description="è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼")

class CalculatorTool(BaseTool):
    name = "calculator"
    description = "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"
    args_schema = CalculatorArgs

    async def run(self, expression: str) -> str:
        try:
            result = eval(expression)  # ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨å®‰å…¨çš„æ–¹å¼
            return f"ç»“æœ: {result}"
        except Exception as e:
            return f"é”™è¯¯: {str(e)}"

# ä½¿ç”¨
my_agent = agent(
    provider="openai",
    model="gpt-4",
    tools=[CalculatorTool()]
)
```

### å¼‚æ­¥å·¥å…·

```python
import aiohttp
from loom import tool

@tool()
async def fetch_url(url: str) -> str:
    """å¼‚æ­¥è·å–URLå†…å®¹"""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

my_agent = agent(
    provider="openai",
    model="gpt-4",
    tools=[fetch_url()]
)
```

### å·¥å…·å¹¶å‘æ‰§è¡Œ

v4.0.0æ”¯æŒè‡ªåŠ¨å¹¶å‘æ‰§è¡Œå·¥å…·ï¼ˆ10xæ€§èƒ½æå‡ï¼‰ï¼š

```python
@tool(concurrency_safe=True)  # é»˜è®¤ä¸ºTrue
def read_file(path: str) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆå¹¶å‘å®‰å…¨ï¼‰"""
    with open(path) as f:
        return f.read()

@tool(concurrency_safe=False)  # å†™æ“ä½œä¸å¹¶å‘
def write_file(path: str, content: str) -> str:
    """å†™å…¥æ–‡ä»¶å†…å®¹"""
    with open(path, 'w') as f:
        f.write(content)
    return "å†™å…¥æˆåŠŸ"

# Agentä¼šè‡ªåŠ¨å¤„ç†ï¼š
# - readæ“ä½œå¹¶å‘æ‰§è¡Œ
# - writeæ“ä½œä¸²è¡Œæ‰§è¡Œï¼ˆç›¸åŒæ–‡ä»¶ï¼‰
```

---

## RAGæ¨¡å¼

### ä»€ä¹ˆæ˜¯RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é€šè¿‡ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºLLMçš„å›ç­”èƒ½åŠ›ã€‚

### åŸºç¡€RAGç¤ºä¾‹

```python
from loom import agent
from loom.patterns.rag import RAGPattern
from loom.builtin.retrievers import ChromaRetriever

# 1. åˆ›å»ºAgent
my_agent = agent(provider="openai", model="gpt-4")

# 2. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆéœ€è¦å…ˆæ„å»ºå‘é‡æ•°æ®åº“ï¼‰
retriever = ChromaRetriever(
    collection_name="my_docs",
    persist_directory="./chroma_db"
)

# 3. åˆ›å»ºRAG Pattern
rag = RAGPattern(
    agent=my_agent,
    retriever=retriever,
    top_k=5  # æ£€ç´¢å‰5ä¸ªæ–‡æ¡£
)

# 4. ä½¿ç”¨RAGå›ç­”é—®é¢˜
result = await rag.run("Loomæ¡†æ¶çš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ")
print(result)
```

### é«˜çº§RAG: å¤šæŸ¥è¯¢RAG

ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ä»¥æé«˜å¬å›ç‡ï¼š

```python
from loom.patterns.rag import MultiQueryRAG

rag = MultiQueryRAG(
    agent=my_agent,
    retriever=retriever,
    query_count=3,  # ç”Ÿæˆ3ä¸ªæŸ¥è¯¢å˜ä½“
    top_k=10,
    rerank_top_k=3
)

result = await rag.run("å¦‚ä½•ä¼˜åŒ–Agentæ€§èƒ½ï¼Ÿ")
```

### å±‚æ¬¡åŒ–RAG

å…ˆæ£€ç´¢æ–‡æ¡£ï¼Œå†æ£€ç´¢æ®µè½ï¼š

```python
from loom.patterns.rag import HierarchicalRAG

rag = HierarchicalRAG(
    agent=my_agent,
    document_retriever=doc_retriever,
    paragraph_retriever=para_retriever,
    doc_top_k=5,
    para_top_k=3
)

result = await rag.run("è¯¦ç»†è§£é‡Šé”™è¯¯å¤„ç†æœºåˆ¶")
```

### ä½¿ç”¨RAGå·¥å…·

å°†RAGä½œä¸ºå·¥å…·é›†æˆåˆ°Agentï¼š

```python
from loom import tool

@tool()
async def search_docs(query: str) -> str:
    """æœç´¢æ–‡æ¡£çŸ¥è¯†åº“"""
    docs = await retriever.retrieve(query, top_k=3)
    return "\n\n".join([doc.content for doc in docs])

my_agent = agent(
    provider="openai",
    model="gpt-4",
    tools=[search_docs()],
    system_instructions="å½“éœ€è¦æŸ¥æ‰¾æ–‡æ¡£æ—¶ï¼Œä½¿ç”¨search_docså·¥å…·ã€‚"
)

result = await my_agent.run("æŸ¥æ‰¾å…³äºå‹ç¼©ç®¡ç†å™¨çš„æ–‡æ¡£")
```

---

## å†…å­˜ç®¡ç†

### å†…å­˜ç±»å‹

#### 1. InMemoryMemoryï¼ˆé»˜è®¤ï¼‰

è¿›ç¨‹å†…ä¸´æ—¶å†…å­˜ï¼Œè¿›ç¨‹ç»“æŸåä¸¢å¤±ï¼š

```python
from loom import Agent
from loom.builtin.memory import InMemoryMemory

memory = InMemoryMemory()

my_agent = Agent(llm=llm, memory=memory)
```

#### 2. PersistentMemoryï¼ˆæ¨èï¼‰

è·¨ä¼šè¯æŒä¹…åŒ–å†…å­˜ï¼Œè‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜ï¼š

```python
from loom import Agent
from loom.builtin.memory import PersistentMemory

memory = PersistentMemory(
    persist_dir=".loom",          # ä¿å­˜ç›®å½•
    session_id="user_123",        # ä¼šè¯IDï¼ˆå¯é€‰ï¼‰
    enable_persistence=True,      # å¯ç”¨æŒä¹…åŒ–
    auto_backup=True,             # è‡ªåŠ¨å¤‡ä»½
    max_backup_files=5            # æœ€å¤šä¿ç•™5ä¸ªå¤‡ä»½
)

my_agent = Agent(llm=llm, memory=memory)

# å¯¹è¯ä¼šè‡ªåŠ¨ä¿å­˜
await my_agent.run("ä½ å¥½")
await my_agent.run("æˆ‘åˆšæ‰è¯´äº†ä»€ä¹ˆï¼Ÿ")  # è®°å¾—ä¸Šä¸€å¥

# é‡å¯ç¨‹åºåï¼Œä½¿ç”¨ç›¸åŒsession_idå¯ä»¥æ¢å¤å¯¹è¯
```

### å†…å­˜æ“ä½œ

```python
# è·å–æ‰€æœ‰æ¶ˆæ¯
messages = memory.get_messages()

# æ·»åŠ æ¶ˆæ¯
from loom.core.types import Message, MessageRole

memory.add_message(Message(
    role=MessageRole.USER,
    content="Hello"
))

# æ¸…ç©ºå†…å­˜
memory.clear()

# è·å–æŒä¹…åŒ–ä¿¡æ¯
info = memory.get_persistence_info()
print(f"ä¼šè¯æ–‡ä»¶: {info['session_file']}")
print(f"å¤‡ä»½æ•°é‡: {info['backup_count']}")
```

### ä¸Šä¸‹æ–‡å‹ç¼©

v4.0.0è‡ªåŠ¨å‹ç¼©åŠŸèƒ½ï¼ˆ92%é˜ˆå€¼è§¦å‘ï¼‰ï¼š

```python
from loom import Agent

# é»˜è®¤å·²å¯ç”¨å‹ç¼©ï¼Œæ— éœ€é¢å¤–é…ç½®
my_agent = Agent(
    llm=llm,
    max_context_tokens=16000  # è¾¾åˆ°92%æ—¶è‡ªåŠ¨å‹ç¼©
)

# å‹ç¼©åå¯ä»¥è¿›è¡Œ5å€æ›´é•¿çš„å¯¹è¯
for i in range(100):
    await my_agent.run(f"è¿™æ˜¯ç¬¬{i}è½®å¯¹è¯")
    # Agentä¼šè‡ªåŠ¨å‹ç¼©å†å²ï¼Œä¿æŒä¸Šä¸‹æ–‡åœ¨é™åˆ¶å†…
```

---

## ç”Ÿäº§ç¯å¢ƒé…ç½®

### åŸºç¡€ç”Ÿäº§é…ç½®

```python
from loom import Agent
from loom.builtin.memory import PersistentMemory
from loom.callbacks.observability import ObservabilityCallback, MetricsAggregator

# 1. æŒä¹…åŒ–å†…å­˜
memory = PersistentMemory()

# 2. å¯è§‚æµ‹æ€§å›è°ƒ
obs_callback = ObservabilityCallback()
metrics = MetricsAggregator()

# 3. åˆ›å»ºç”Ÿäº§çº§Agent
my_agent = Agent(
    llm=llm,
    memory=memory,
    callbacks=[obs_callback, metrics],
    max_iterations=50,
    max_context_tokens=16000
)

# 4. è¿è¡Œ
result = await my_agent.run("å¤„ç†ç”¨æˆ·è¯·æ±‚")

# 5. æŸ¥çœ‹æŒ‡æ ‡
summary = metrics.get_summary()
print(f"LLMè°ƒç”¨æ¬¡æ•°: {summary['llm_calls']}")
print(f"å¹³å‡å»¶è¿Ÿ: {summary.get('avg_llm_latency_ms', 0):.2f}ms")
print(f"é”™è¯¯ç‡: {summary.get('errors_per_minute', 0):.2f}/min")
```

### ä¼ä¸šçº§é…ç½®ï¼ˆå®Œæ•´åŠŸèƒ½æ ˆï¼‰

```python
from loom import (
    Agent,
    PersistentMemory,
    ModelPoolLLM,
    ModelConfig,
    ObservabilityCallback,
    MetricsAggregator,
    RetryPolicy,
    CircuitBreaker,
)
from loom.builtin.llms import OpenAILLM

# 1. åˆ›å»ºä¸»LLMå’Œå¤‡ç”¨LLM
gpt4 = OpenAILLM(api_key="sk-...", model="gpt-4")
gpt35 = OpenAILLM(api_key="sk-...", model="gpt-3.5-turbo")

# 2. é…ç½®æ¨¡å‹æ± ï¼ˆè‡ªåŠ¨æ•…éšœè½¬ç§»ï¼‰
pool_llm = ModelPoolLLM([
    ModelConfig("gpt-4", gpt4, priority=100),      # ä¸»æ¨¡å‹
    ModelConfig("gpt-3.5", gpt35, priority=50),    # å¤‡ç”¨æ¨¡å‹
])

# 3. æŒä¹…åŒ–å†…å­˜
memory = PersistentMemory(
    session_id="production_session",
    max_backup_files=10
)

# 4. å¯è§‚æµ‹æ€§
obs = ObservabilityCallback()
metrics = MetricsAggregator()

# 5. å¼¹æ€§ç»„ä»¶
retry_policy = RetryPolicy(
    max_retries=3,
    base_delay=1.0,
    exponential_base=2.0
)
circuit_breaker = CircuitBreaker(
    failure_threshold=5,
    timeout_seconds=60
)

# 6. åˆ›å»ºä¼ä¸šçº§Agent
my_agent = Agent(
    llm=pool_llm,
    memory=memory,
    callbacks=[obs, metrics],
    tools=your_tools,
    system_instructions="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"
)

# 7. ä½¿ç”¨å¼¹æ€§åŒ…è£…æ‰§è¡Œ
async def robust_run(prompt: str) -> str:
    """å¸¦é‡è¯•å’Œç†”æ–­å™¨çš„æ‰§è¡Œ"""
    return await retry_policy.execute_with_retry(
        circuit_breaker.call,
        my_agent.run,
        prompt
    )

# 8. æ‰§è¡Œ
try:
    result = await robust_run("ç”¨æˆ·è¯·æ±‚")
    print(result)
except Exception as e:
    print(f"è¯·æ±‚å¤±è´¥: {e}")

# 9. ç›‘æ§å¥åº·çŠ¶å†µ
health = pool_llm.get_health_summary()
for model_id, status in health.items():
    print(f"{model_id}: {status['status']} (æˆåŠŸç‡: {status['success_rate']*100:.1f}%)")
```

### ç»“æ„åŒ–æ—¥å¿—

```python
from loom.core.structured_logger import get_logger, set_correlation_id

# åˆ›å»ºlogger
logger = get_logger("my_app")

# è®¾ç½®å…³è”IDï¼ˆè·¨è¯·æ±‚è¿½è¸ªï¼‰
set_correlation_id("req-12345")

# æ‰€æœ‰æ—¥å¿—è‡ªåŠ¨åŒ…å«å…³è”IDå’ŒJSONæ ¼å¼
logger.info("å¤„ç†è¯·æ±‚", user_id="user_456", action="query")
# è¾“å‡º: {"timestamp": "...", "level": "INFO", "correlation_id": "req-12345", ...}

# æ€§èƒ½æ—¥å¿—
logger.log_performance("llm_call", duration_ms=234.5, tokens=150)
```

---

## é«˜çº§ç‰¹æ€§

### 1. å®æ—¶å–æ¶ˆï¼ˆv4.0.0ï¼‰

```python
import asyncio

cancel_token = asyncio.Event()

# å¯åŠ¨é•¿æ—¶é—´ä»»åŠ¡
task = asyncio.create_task(
    my_agent.run("æ‰§è¡Œé•¿æ—¶é—´åˆ†æ", cancel_token=cancel_token)
)

# 5ç§’åå–æ¶ˆ
await asyncio.sleep(5)
cancel_token.set()  # è§¦å‘å–æ¶ˆ

try:
    result = await task
except asyncio.CancelledError:
    print("ä»»åŠ¡å·²å–æ¶ˆ")
```

### 2. å­Agentéš”ç¦»

```python
from loom.core.subagent_pool import SubAgentPool

pool = SubAgentPool(max_depth=3)

# åˆ›å»ºéš”ç¦»çš„å­Agent
result = await pool.spawn(
    llm=llm,
    prompt="åˆ†æè¿™æ®µä»£ç ",
    tool_whitelist=["read_file", "analyze_code"],  # åªå…è®¸è¿™äº›å·¥å…·
    timeout_seconds=60,
    max_iterations=20
)

# å­Agentå¤±è´¥ä¸ä¼šå½±å“ä¸»Agent
```

### 3. å¹¶å‘å­Agent

```python
# å¹¶å‘æ‰§è¡Œå¤šä¸ªå­Agent
results = await pool.spawn_many([
    {"llm": llm, "prompt": "ä»»åŠ¡1", "tool_whitelist": ["tool1"]},
    {"llm": llm, "prompt": "ä»»åŠ¡2", "tool_whitelist": ["tool2"]},
    {"llm": llm, "prompt": "ä»»åŠ¡3", "tool_whitelist": ["tool3"]},
])

for i, result in enumerate(results):
    print(f"ä»»åŠ¡{i+1}ç»“æœ: {result}")
```

### 4. ç³»ç»Ÿæé†’

è‡ªåŠ¨ç”Ÿæˆè¿è¡Œæ—¶æç¤ºï¼š

```python
from loom.core.system_reminders import SystemReminderManager

manager = SystemReminderManager()

# æ£€æŸ¥å½“å‰çŠ¶æ€
context = {
    "current_tokens": 15000,
    "max_tokens": 16000,
    "metrics": {"total_errors": 3, "total_operations": 10}
}

reminders = manager.check_all(context)
# è¾“å‡º: ["âš ï¸ WARNING: Memory usage is high (93.8%)", ...]

# è‡ªåŠ¨æ³¨å…¥åˆ°ç³»ç»Ÿæç¤º
enhanced_prompt = manager.inject_into_context(context, system_prompt)
```

### 5. æƒé™æ§åˆ¶

```python
my_agent = agent(
    provider="openai",
    model="gpt-4",
    tools=[read_file(), write_file(), delete_file()],
    permission_policy={
        "read_file": "allow",
        "write_file": "ask",     # éœ€è¦ç”¨æˆ·ç¡®è®¤
        "delete_file": "deny"    # ç¦æ­¢
    },
    safe_mode=True  # å¯ç”¨å®‰å…¨æ¨¡å¼
)
```

### 6. æµå¼è¾“å‡º

```python
async for event in my_agent.stream("è®²ä¸ªé•¿æ•…äº‹"):
    if event.type == "text":
        print(event.content, end="", flush=True)
    elif event.type == "tool_call":
        print(f"\n[è°ƒç”¨å·¥å…·: {event.tool_name}]")
    elif event.type == "tool_result":
        print(f"[å·¥å…·è¿”å›: {event.result}]")
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„LLMï¼Ÿ

**ç­”**: æ ¹æ®ä»»åŠ¡é€‰æ‹©ï¼š
- **GPT-4**: å¤æ‚æ¨ç†ã€ä»£ç ç”Ÿæˆã€é«˜è´¨é‡è¾“å‡º
- **GPT-3.5-turbo**: å¿«é€Ÿå“åº”ã€æˆæœ¬ä¼˜åŒ–
- **Claude**: é•¿æ–‡æœ¬å¤„ç†ã€å®‰å…¨å¯¹è¯
- **Ollama**: æœ¬åœ°éƒ¨ç½²ã€éšç§ä¿æŠ¤

### Q2: å¦‚ä½•ä¼˜åŒ–æˆæœ¬ï¼Ÿ

**ç­”**:
1. ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚gpt-3.5-turboï¼‰
2. å¯ç”¨ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
3. è®¾ç½®åˆç†çš„`max_tokens`
4. ä½¿ç”¨æ¨¡å‹æ± åœ¨ä¸»/å¤‡ç”¨æ¨¡å‹é—´åˆ‡æ¢

```python
pool_llm = ModelPoolLLM([
    ModelConfig("gpt-4", gpt4, priority=100),      # å¤æ‚ä»»åŠ¡
    ModelConfig("gpt-3.5", gpt35, priority=50),    # ç®€å•ä»»åŠ¡
])
```

### Q3: Agentæ— å“åº”æ€ä¹ˆåŠï¼Ÿ

**ç­”**:
1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥ç½‘ç»œè¿æ¥
3. ä½¿ç”¨å–æ¶ˆä»¤ç‰Œè®¾ç½®è¶…æ—¶
4. æŸ¥çœ‹æ—¥å¿—æ’æŸ¥é—®é¢˜

```python
import asyncio

cancel_token = asyncio.Event()

async def timeout_wrapper():
    task = asyncio.create_task(
        my_agent.run("query", cancel_token=cancel_token)
    )
    try:
        return await asyncio.wait_for(task, timeout=30.0)
    except asyncio.TimeoutError:
        cancel_token.set()
        raise
```

### Q4: å¦‚ä½•å¤„ç†å¤§æ–‡ä»¶ï¼Ÿ

**ç­”**: ä½¿ç”¨RAGæ¨¡å¼å°†æ–‡ä»¶åˆ‡å—å­˜å…¥å‘é‡æ•°æ®åº“ï¼š

```python
from loom.builtin.retrievers import ChromaRetriever

# 1. æ„å»ºç´¢å¼•
retriever = ChromaRetriever(collection_name="docs")
await retriever.add_documents([
    Document(content=chunk1, metadata={"source": "file1.txt"}),
    Document(content=chunk2, metadata={"source": "file1.txt"}),
])

# 2. ä½¿ç”¨RAGæŸ¥è¯¢
rag = RAGPattern(agent=my_agent, retriever=retriever)
result = await rag.run("æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### Q5: å¦‚ä½•è°ƒè¯•Agentï¼Ÿ

**ç­”**:
1. å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

2. ä½¿ç”¨å›è°ƒæŸ¥çœ‹äº‹ä»¶ï¼š
```python
from loom.callbacks.base import BaseCallback

class DebugCallback(BaseCallback):
    async def on_llm_start(self, messages, **kwargs):
        print(f"LLMè¾“å…¥: {messages}")

    async def on_llm_end(self, response, **kwargs):
        print(f"LLMè¾“å‡º: {response}")

my_agent = Agent(llm=llm, callbacks=[DebugCallback()])
```

3. æŸ¥çœ‹æŒ‡æ ‡ï¼š
```python
summary = metrics.get_summary()
print(summary)
```

### Q6: å¦‚ä½•å®ç°å¤šè½®å¯¹è¯ï¼Ÿ

**ç­”**: ä½¿ç”¨æŒä¹…åŒ–å†…å­˜è‡ªåŠ¨ç®¡ç†ï¼š

```python
from loom.builtin.memory import PersistentMemory

memory = PersistentMemory(session_id="user_123")
my_agent = Agent(llm=llm, memory=memory)

# ç¬¬1è½®
await my_agent.run("æˆ‘å«å¼ ä¸‰")

# ç¬¬2è½®
result = await my_agent.run("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
# è¾“å‡º: "ä½ å«å¼ ä¸‰"
```

### Q7: å¦‚ä½•é›†æˆåˆ°Webåº”ç”¨ï¼Ÿ

**ç­”**: ä½¿ç”¨FastAPIç¤ºä¾‹ï¼š

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class QueryRequest(BaseModel):
    query: str
    session_id: str

@app.post("/chat")
async def chat(request: QueryRequest):
    try:
        memory = PersistentMemory(session_id=request.session_id)
        agent = Agent(llm=llm, memory=memory)
        result = await agent.run(request.query)
        return {"response": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Q8: å¦‚ä½•å®ç°æµå¼å“åº”åˆ°å‰ç«¯ï¼Ÿ

**ç­”**: ä½¿ç”¨SSEï¼ˆServer-Sent Eventsï¼‰ï¼š

```python
from fastapi.responses import StreamingResponse

@app.get("/chat/stream")
async def chat_stream(query: str):
    async def event_generator():
        async for event in my_agent.stream(query):
            if event.type == "text":
                yield f"data: {event.content}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
```

---

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ [V4_FINAL_SUMMARY.md](V4_FINAL_SUMMARY.md) äº†è§£v4.0.0æ‰€æœ‰ç‰¹æ€§
- æŸ¥çœ‹ [examples/](examples/) ç›®å½•è·å–æ›´å¤šç¤ºä¾‹
- æŸ¥çœ‹ [CHANGELOG.md](CHANGELOG.md) äº†è§£ç‰ˆæœ¬å˜åŒ–
- æŸ¥çœ‹ [P2_FEATURES.md](P2_FEATURES.md) äº†è§£ç”Ÿäº§çº§ç‰¹æ€§

---

## è·å–å¸®åŠ©

- GitHub Issues: [https://github.com/your-org/loom-agent/issues](https://github.com/your-org/loom-agent/issues)
- æ–‡æ¡£: [https://docs.loom-agent.dev](https://docs.loom-agent.dev)

---

**Happy Coding with Loom Agent v4.0.0!** ğŸ‰
