# å†…ç½® Skills

**ç‰ˆæœ¬**: v0.1.6
**éš¾åº¦**: åˆçº§

äº†è§£å’Œä½¿ç”¨ Loom Agent å†…ç½®çš„ Skillsã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [PDF Analyzer](#pdf-analyzer)
3. [Web Research](#web-research)
4. [Data Processor](#data-processor)
5. [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
6. [å¸¸è§ç”¨ä¾‹](#å¸¸è§ç”¨ä¾‹)
7. [å¯ç”¨å’Œç¦ç”¨](#å¯ç”¨å’Œç¦ç”¨)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ¦‚è¿°

Loom Agent v0.1.6 å†…ç½® **3 ä¸ª** Skillsï¼Œå¼€ç®±å³ç”¨ï¼š

| Skill | åˆ†ç±» | åŠŸèƒ½ | ä¾èµ– |
|-------|------|------|------|
| **pdf_analyzer** | analysis | PDF æ–‡æ¡£åˆ†æä¸æå– | PyPDF2, pdfplumber |
| **web_research** | tools | Web ç ”ç©¶å’Œä¿¡æ¯æ”¶é›† | requests, beautifulsoup4 |
| **data_processor** | tools | ç»“æ„åŒ–æ•°æ®å¤„ç† | pandas, openpyxl |

### è‡ªåŠ¨é›†æˆ

```python
import loom
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="assistant",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True,        # âœ… é»˜è®¤å¯ç”¨
    skills_dir="./skills"      # âœ… é»˜è®¤è·¯å¾„
)

# 3 ä¸ª Skills è‡ªåŠ¨åŠ è½½åˆ°ç³»ç»Ÿæç¤º
# Agent å¯ä»¥æŒ‰éœ€ä½¿ç”¨
```

### ä¸‰å±‚æ¸è¿›å¼æŠ«éœ²

æ¯ä¸ª Skill éƒ½é‡‡ç”¨ä¸‰å±‚æ¶æ„ï¼š

```
ç¬¬ä¸€å±‚ï¼ˆç´¢å¼•ï¼‰â†’ ç³»ç»Ÿæç¤ºï¼Œ~50 tokens
ç¬¬äºŒå±‚ï¼ˆè¯¦ç»†æ–‡æ¡£ï¼‰â†’ SKILL.mdï¼ŒæŒ‰éœ€åŠ è½½ï¼Œ~500-2000 tokens
ç¬¬ä¸‰å±‚ï¼ˆèµ„æºæ–‡ä»¶ï¼‰â†’ resources/ï¼ŒæŒ‰éœ€è®¿é—®ï¼Œä»»æ„å¤§å°
```

**ä¼˜åŠ¿**ï¼šæœ€å°åŒ–ä¸Šä¸‹æ–‡ä½¿ç”¨ï¼ŒAgent éœ€è¦æ—¶æ‰åŠ è½½è¯¦ç»†ä¿¡æ¯ã€‚

---

## PDF Analyzer

### æ¦‚è¿°

**åç§°**: `pdf_analyzer`
**åˆ†ç±»**: `analysis`
**ç‰ˆæœ¬**: `1.0.0`

**åŠŸèƒ½**ï¼š
- æ–‡æœ¬æå–
- è¡¨æ ¼æå–
- å…ƒæ•°æ®æå–
- é€é¡µå¤„ç†
- OCR æ”¯æŒï¼ˆå¯é€‰ï¼‰

### ç¬¬ä¸€å±‚ï¼šç´¢å¼•

Agent åœ¨ç³»ç»Ÿæç¤ºä¸­çœ‹åˆ°ï¼š

```
## Analysis

- **pdf_analyzer**: Analyze and extract information from PDF documents
  ğŸ’¡ Quick: Use PyPDF2 or pdfplumber to extract text, tables, and metadata from PDF files. Check resources/examples.json for common patterns.
  ğŸ“„ Details: `cat skills/pdf_analyzer/SKILL.md`
  ğŸ“¦ Resources: `ls skills/pdf_analyzer/resources/`
```

### ç¬¬äºŒå±‚ï¼šè¯¦ç»†æ–‡æ¡£

Agent éœ€è¦æ—¶ä¼šæ‰§è¡Œï¼š
```bash
cat skills/pdf_analyzer/SKILL.md
```

å†…å®¹åŒ…æ‹¬ï¼š
- **æ–‡æœ¬æå–**ï¼šä½¿ç”¨ PyPDF2 æå–çº¯æ–‡æœ¬
- **è¡¨æ ¼æå–**ï¼šä½¿ç”¨ pdfplumber æå–è¡¨æ ¼æ•°æ®
- **å…ƒæ•°æ®æå–**ï¼šè·å–ä½œè€…ã€æ ‡é¢˜ã€åˆ›å»ºæ—¥æœŸç­‰
- **OCR å¤„ç†**ï¼šå¯¹æ‰«æç‰ˆ PDF ä½¿ç”¨ pytesseract

### ç¬¬ä¸‰å±‚ï¼šèµ„æºæ–‡ä»¶

```
skills/pdf_analyzer/resources/
  examples.json        # å¸¸è§ä½¿ç”¨æ¨¡å¼ç¤ºä¾‹
```

### æ ¸å¿ƒèƒ½åŠ›

#### 1. æ–‡æœ¬æå–

```python
import PyPDF2

def extract_text(pdf_path: str) -> str:
    """ä» PDF æå–æ‰€æœ‰æ–‡æœ¬"""
    with open(pdf_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text
```

#### 2. è¡¨æ ¼æå–

```python
import pdfplumber

def extract_tables(pdf_path: str) -> list:
    """æå– PDF ä¸­çš„æ‰€æœ‰è¡¨æ ¼"""
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            tables.extend(page.extract_tables())
    return tables
```

#### 3. å…ƒæ•°æ®æå–

```python
def extract_metadata(pdf_path: str) -> dict:
    """æå– PDF å…ƒæ•°æ®"""
    with open(pdf_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        metadata = reader.metadata
    return {
        'author': metadata.get('/Author'),
        'title': metadata.get('/Title'),
        'subject': metadata.get('/Subject'),
        'creator': metadata.get('/Creator'),
        'pages': len(reader.pages)
    }
```

### ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | ç¤ºä¾‹ä»»åŠ¡ |
|------|----------|
| **æ–‡æ¡£å¤„ç†** | "æå–è¿™ä»½åˆåŒçš„å…³é”®æ¡æ¬¾" |
| **å‘ç¥¨åˆ†æ** | "ä»è¿™ 10 å¼ å‘ç¥¨ä¸­æå–é‡‘é¢å’Œæ—¥æœŸ" |
| **ç®€å†è§£æ** | "ä»ç®€å†ä¸­æå–å€™é€‰äººçš„å·¥ä½œç»éªŒ" |
| **æŠ¥å‘Šæ±‡æ€»** | "æ±‡æ€»è¿™ä»½ PDF æŠ¥å‘Šçš„ä¸»è¦ç»“è®º" |

### ä¾èµ–å®‰è£…

```bash
# åŸºç¡€åŠŸèƒ½
pip install PyPDF2 pdfplumber

# OCR åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
pip install pytesseract pdf2image
# macOS: brew install tesseract
# Ubuntu: sudo apt-get install tesseract-ocr
```

### ç¤ºä¾‹ï¼šå®Œæ•´ PDF åˆ†æ

```python
from loom import Message, SimpleAgent
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="pdf-analyst",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

# ä»»åŠ¡
msg = Message(
    role="user",
    content="""
    åˆ†æ report.pdfï¼Œæå–ï¼š
    1. æ–‡æ¡£æ ‡é¢˜å’Œä½œè€…
    2. æ‰€æœ‰è¡¨æ ¼æ•°æ®
    3. å…³é”®ç»“è®ºï¼ˆåœ¨ç¬¬ 5 é¡µï¼‰
    """
)

response = await agent.run(msg)
print(response.content)

# Agent ä¼šï¼š
# 1. è¯†åˆ« pdf_analyzer skill
# 2. è¯»å– SKILL.md äº†è§£è¯¦ç»†ç”¨æ³•
# 3. ä½¿ç”¨ PyPDF2 æå–å…ƒæ•°æ®
# 4. ä½¿ç”¨ pdfplumber æå–è¡¨æ ¼
# 5. æå–ç¬¬ 5 é¡µçš„æ–‡æœ¬å¹¶æ€»ç»“
```

---

## Web Research

### æ¦‚è¿°

**åç§°**: `web_research`
**åˆ†ç±»**: `tools`
**ç‰ˆæœ¬**: `1.0.0`

**åŠŸèƒ½**ï¼š
- æœç´¢å¼•æ“æŸ¥è¯¢ï¼ˆGoogle, Bing, DuckDuckGoï¼‰
- Web æŠ“å–å’Œå†…å®¹æå–
- åŠ¨æ€å†…å®¹å¤„ç†ï¼ˆJavaScript æ¸²æŸ“é¡µé¢ï¼‰
- å¤šæºä¿¡æ¯ç»¼åˆ
- å¼•ç”¨å’Œæ¥æºè¿½è¸ª

### ç¬¬ä¸€å±‚ï¼šç´¢å¼•

```
## Tools

- **web_research**: Conduct web research and gather information from online sources
  ğŸ’¡ Quick: Use search APIs (Google, Bing) for queries, requests/beautifulsoup4 for scraping, and selenium for dynamic content. See resources/search_templates.json for query patterns.
  ğŸ“„ Details: `cat skills/web_research/SKILL.md`
  ğŸ“¦ Resources: `ls skills/web_research/resources/`
```

### ç¬¬äºŒå±‚ï¼šè¯¦ç»†æ–‡æ¡£

åŒ…å«ï¼š
- **æœç´¢å¼•æ“æŸ¥è¯¢**ï¼šå¦‚ä½•æ„å»ºæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢
- **Web æŠ“å–**ï¼šä½¿ç”¨ requests å’Œ BeautifulSoup
- **åŠ¨æ€å†…å®¹**ï¼šä½¿ç”¨ Selenium å¤„ç† JavaScript
- **å¤šæºç ”ç©¶**ï¼šç»¼åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯

### ç¬¬ä¸‰å±‚ï¼šèµ„æºæ–‡ä»¶

```
skills/web_research/resources/
  search_templates.json    # æœç´¢æŸ¥è¯¢æ¨¡æ¿
```

### æ ¸å¿ƒèƒ½åŠ›

#### 1. æœç´¢å¼•æ“æŸ¥è¯¢

```python
import requests
from bs4 import BeautifulSoup

def google_search(query: str, num_results: int = 10) -> list:
    """æ‰§è¡Œ Google æœç´¢å¹¶è¿”å›ç»“æœ"""
    url = f"https://www.google.com/search?q={query}&num={num_results}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    results = []
    for g in soup.find_all('div', class_='g'):
        title = g.find('h3')
        link = g.find('a')
        if title and link:
            results.append({
                'title': title.get_text(),
                'link': link.get('href')
            })
    return results
```

#### 2. Web æŠ“å–

```python
def scrape_article(url: str) -> dict:
    """ä» URL æå–æ–‡ç« å†…å®¹"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # æå–ä¸»è¦å†…å®¹
    article = soup.find('article') or soup.find('main')

    return {
        'title': soup.find('h1').get_text() if soup.find('h1') else '',
        'content': article.get_text() if article else '',
        'url': url
    }
```

#### 3. åŠ¨æ€å†…å®¹å¤„ç†

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def scrape_dynamic_page(url: str) -> str:
    """æŠ“å– JavaScript æ¸²æŸ“çš„å†…å®¹"""
    driver = webdriver.Chrome()
    driver.get(url)

    # ç­‰å¾…å†…å®¹åŠ è½½
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "content"))
    )

    content = driver.find_element(By.CLASS_NAME, "content").text
    driver.quit()

    return content
```

#### 4. å¤šæºç ”ç©¶

```python
def research_topic(topic: str, num_sources: int = 5) -> dict:
    """ä»å¤šä¸ªæ¥æºç ”ç©¶ä¸»é¢˜"""
    # 1. æœç´¢æ¥æº
    search_results = google_search(topic, num_sources)

    # 2. æå–å†…å®¹
    sources = []
    for result in search_results:
        try:
            content = scrape_article(result['link'])
            sources.append(content)
        except Exception as e:
            print(f"Failed to scrape {result['link']}: {e}")

    # 3. ç»¼åˆä¿¡æ¯
    return {
        'topic': topic,
        'num_sources': len(sources),
        'sources': sources
    }
```

### ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | ç¤ºä¾‹ä»»åŠ¡ |
|------|----------|
| **å¸‚åœºç ”ç©¶** | "ç ”ç©¶ AI Agent å¸‚åœºçš„æœ€æ–°è¶‹åŠ¿" |
| **ç«å“åˆ†æ** | "å¯¹æ¯” 3 ä¸ªä¸»æµé¡¹ç›®ç®¡ç†å·¥å…·çš„ç‰¹æ€§" |
| **äº‹å®æ ¸æŸ¥** | "éªŒè¯è¿™æ¡æ–°é—»çš„çœŸå®æ€§" |
| **è¶‹åŠ¿åˆ†æ** | "åˆ†ææœ€è¿‘ 6 ä¸ªæœˆ React æ¡†æ¶çš„å‘å±•" |

### ä¾èµ–å®‰è£…

```bash
# åŸºç¡€åŠŸèƒ½
pip install requests beautifulsoup4

# åŠ¨æ€å†…å®¹ï¼ˆå¯é€‰ï¼‰
pip install selenium
# éœ€è¦ä¸‹è½½ ChromeDriver: https://chromedriver.chromium.org/
```

### ç¤ºä¾‹ï¼šå¸‚åœºç ”ç©¶

```python
from loom import Message, SimpleAgent
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="researcher",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

msg = Message(
    role="user",
    content="""
    ç ”ç©¶"AI Agent æ¡†æ¶"è¿™ä¸ªä¸»é¢˜ï¼š
    1. æ‰¾åˆ° 5 ä¸ªç›¸å…³çš„æŠ€æœ¯æ–‡ç« 
    2. æ€»ç»“ä¸»è¦è¶‹åŠ¿
    3. åˆ—å‡ºä¸»æµæ¡†æ¶çš„ä¼˜ç¼ºç‚¹
    """
)

response = await agent.run(msg)
print(response.content)

# Agent ä¼šï¼š
# 1. è¯†åˆ« web_research skill
# 2. è¯»å– SKILL.md å’Œæœç´¢æ¨¡æ¿
# 3. æ‰§è¡Œ Google æœç´¢
# 4. æŠ“å–æ–‡ç« å†…å®¹
# 5. ç»¼åˆåˆ†æå¹¶ç»™å‡ºæŠ¥å‘Š
```

### æ³¨æ„äº‹é¡¹

- **éµå®ˆ robots.txt**ï¼šå§‹ç»ˆæ£€æŸ¥ç½‘ç«™çš„çˆ¬è™«æ”¿ç­–
- **é€Ÿç‡é™åˆ¶**ï¼šé¿å…é¢‘ç¹è¯·æ±‚ï¼Œæ·»åŠ å»¶è¿Ÿ
- **User-Agent**ï¼šä½¿ç”¨åˆé€‚çš„ User-Agent å¤´
- **ä»£ç†**ï¼šå¤§è§„æ¨¡æŠ“å–æ—¶è€ƒè™‘ä½¿ç”¨ä»£ç†
- **ç¼“å­˜**ï¼šç¼“å­˜ç»“æœä»¥å‡å°‘é‡å¤è¯·æ±‚

---

## Data Processor

### æ¦‚è¿°

**åç§°**: `data_processor`
**åˆ†ç±»**: `tools`
**ç‰ˆæœ¬**: `1.0.0`

**åŠŸèƒ½**ï¼š
- CSV/Excel æ–‡ä»¶è¯»å†™
- JSON æ•°æ®æ“ä½œ
- æ•°æ®æ¸…æ´—å’ŒéªŒè¯
- æ•°æ®è½¬æ¢å’Œèšåˆ
- æ ¼å¼è½¬æ¢
- æ•°æ®è´¨é‡åˆ†æ

### ç¬¬ä¸€å±‚ï¼šç´¢å¼•

```
## Tools

- **data_processor**: Process and transform structured data (CSV, JSON, Excel)
  ğŸ’¡ Quick: Use pandas for tabular data, json module for JSON, and openpyxl for Excel. Check resources/transformation_patterns.json for common operations like filtering, aggregation, and merging.
  ğŸ“„ Details: `cat skills/data_processor/SKILL.md`
  ğŸ“¦ Resources: `ls skills/data_processor/resources/`
```

### ç¬¬äºŒå±‚ï¼šè¯¦ç»†æ–‡æ¡£

åŒ…å«ï¼š
- **CSV å¤„ç†**ï¼šè¯»å–ã€æ¸…æ´—ã€è½¬æ¢ã€ä¿å­˜
- **JSON å¤„ç†**ï¼šè§£æã€è½¬æ¢ã€æ‰å¹³åŒ–
- **æ•°æ®èšåˆ**ï¼šåˆ†ç»„ã€ç»Ÿè®¡ã€è®¡ç®—
- **æ•°æ®åˆå¹¶**ï¼šå¤šæ•°æ®é›†åˆå¹¶
- **æ•°æ®éªŒè¯**ï¼šè§„åˆ™æ£€æŸ¥ã€è´¨é‡åˆ†æ
- **æ ¼å¼è½¬æ¢**ï¼šCSV â†” JSON â†” Excel

### ç¬¬ä¸‰å±‚ï¼šèµ„æºæ–‡ä»¶

```
skills/data_processor/resources/
  transformation_patterns.json    # å¸¸è§æ•°æ®è½¬æ¢æ¨¡å¼
```

### æ ¸å¿ƒèƒ½åŠ›

#### 1. CSV å¤„ç†

```python
import pandas as pd

def process_csv(input_path: str, output_path: str) -> dict:
    """è¯»å–ã€å¤„ç†å¹¶ä¿å­˜ CSV æ•°æ®"""
    # è¯»å– CSV
    df = pd.read_csv(input_path)

    # åŸºç¡€æ¸…æ´—
    df = df.drop_duplicates()
    df = df.dropna(subset=['important_column'])

    # è½¬æ¢
    df['new_column'] = df['col1'] + df['col2']

    # ä¿å­˜ç»“æœ
    df.to_csv(output_path, index=False)

    return {
        'rows_processed': len(df),
        'columns': list(df.columns),
        'output_file': output_path
    }
```

#### 2. JSON å¤„ç†

```python
import json

def process_json(input_path: str, transform_func=None) -> dict:
    """è¯»å–å¹¶è½¬æ¢ JSON æ•°æ®"""
    with open(input_path, 'r') as f:
        data = json.load(f)

    # åº”ç”¨è½¬æ¢
    if transform_func:
        data = transform_func(data)

    return data

def flatten_json(nested_json: dict, prefix: str = '') -> dict:
    """æ‰å¹³åŒ–åµŒå¥—çš„ JSON ç»“æ„"""
    flattened = {}
    for key, value in nested_json.items():
        new_key = f"{prefix}.{key}" if prefix else key
        if isinstance(value, dict):
            flattened.update(flatten_json(value, new_key))
        else:
            flattened[new_key] = value
    return flattened
```

#### 3. æ•°æ®èšåˆ

```python
def aggregate_data(df: pd.DataFrame, group_by: list, agg_funcs: dict) -> pd.DataFrame:
    """æŒ‰æŒ‡å®šåˆ—èšåˆæ•°æ®"""
    # ç¤ºä¾‹: group_by=['category'], agg_funcs={'sales': 'sum', 'quantity': 'mean'}
    result = df.groupby(group_by).agg(agg_funcs).reset_index()
    return result
```

#### 4. æ•°æ®åˆå¹¶

```python
def merge_datasets(df1: pd.DataFrame, df2: pd.DataFrame,
                  on: str, how: str = 'inner') -> pd.DataFrame:
    """åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†"""
    merged = pd.merge(df1, df2, on=on, how=how)
    return merged
```

#### 5. æ•°æ®éªŒè¯

```python
def validate_data(df: pd.DataFrame, rules: dict) -> dict:
    """æ ¹æ®è§„åˆ™éªŒè¯æ•°æ®"""
    validation_results = {
        'valid': True,
        'errors': []
    }

    # æ£€æŸ¥å¿…éœ€åˆ—
    if 'required_columns' in rules:
        missing = set(rules['required_columns']) - set(df.columns)
        if missing:
            validation_results['valid'] = False
            validation_results['errors'].append(f"Missing columns: {missing}")

    # æ£€æŸ¥æ•°æ®ç±»å‹
    if 'data_types' in rules:
        for col, expected_type in rules['data_types'].items():
            if col in df.columns and df[col].dtype != expected_type:
                validation_results['valid'] = False
                validation_results['errors'].append(
                    f"Column {col} has type {df[col].dtype}, expected {expected_type}"
                )

    return validation_results
```

#### 6. æ ¼å¼è½¬æ¢

```python
def convert_format(input_path: str, output_path: str,
                  input_format: str, output_format: str) -> bool:
    """åœ¨æ•°æ®æ ¼å¼ä¹‹é—´è½¬æ¢"""
    # è¯»å–è¾“å…¥
    if input_format == 'csv':
        df = pd.read_csv(input_path)
    elif input_format == 'json':
        df = pd.read_json(input_path)
    elif input_format == 'excel':
        df = pd.read_excel(input_path)

    # å†™å…¥è¾“å‡º
    if output_format == 'csv':
        df.to_csv(output_path, index=False)
    elif output_format == 'json':
        df.to_json(output_path, orient='records', indent=2)
    elif output_format == 'excel':
        df.to_excel(output_path, index=False)

    return True
```

### ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | ç¤ºä¾‹ä»»åŠ¡ |
|------|----------|
| **æ•°æ®æ¸…æ´—** | "æ¸…æ´—è¿™ä¸ª CSV æ–‡ä»¶ï¼šåˆ é™¤é‡å¤é¡¹å’Œç©ºå€¼" |
| **ETL ç®¡é“** | "ä» 3 ä¸ª Excel æ–‡ä»¶æå–ã€è½¬æ¢å¹¶åŠ è½½åˆ°æ•°æ®åº“" |
| **æ•°æ®èšåˆ** | "æŒ‰ç±»åˆ«æ±‡æ€»é”€å”®æ•°æ®ï¼Œè®¡ç®—æ€»é¢å’Œå¹³å‡å€¼" |
| **æ ¼å¼è½¬æ¢** | "å°†è¿™ä¸ª CSV è½¬æ¢ä¸º JSON" |

### ä¾èµ–å®‰è£…

```bash
# åŸºç¡€åŠŸèƒ½
pip install pandas

# Excel æ”¯æŒ
pip install openpyxl

# é«˜çº§åˆ†æ
pip install numpy
```

### ç¤ºä¾‹ï¼šæ•°æ®åˆ†æç®¡é“

```python
from loom import Message, SimpleAgent
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="data-analyst",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

msg = Message(
    role="user",
    content="""
    å¤„ç† sales_data.csvï¼š
    1. æ¸…æ´—æ•°æ®ï¼ˆåˆ é™¤é‡å¤é¡¹å’Œç©ºå€¼ï¼‰
    2. æŒ‰äº§å“ç±»åˆ«èšåˆé”€å”®é¢
    3. è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡ä»·æ ¼
    4. ä¿å­˜ç»“æœåˆ° sales_summary.xlsx
    """
)

response = await agent.run(msg)
print(response.content)

# Agent ä¼šï¼š
# 1. è¯†åˆ« data_processor skill
# 2. è¯»å– SKILL.md äº†è§£ pandas ç”¨æ³•
# 3. è¯»å–å¹¶æ¸…æ´— CSV
# 4. æ‰§è¡Œèšåˆæ“ä½œ
# 5. ä¿å­˜ä¸º Excel æ ¼å¼
```

### æ€§èƒ½ä¼˜åŒ–

```python
# å¤§æ–‡ä»¶ï¼ˆ>1GBï¼‰ï¼šä½¿ç”¨åˆ†å—
df = pd.read_csv('large_file.csv', chunksize=10000)
for chunk in df:
    process(chunk)

# ä¼˜åŒ–å†…å­˜ï¼šæŒ‡å®šæ•°æ®ç±»å‹
df = pd.read_csv('file.csv', dtype={
    'id': 'int32',
    'value': 'float32'
})

# æ—¥æœŸè§£æ
df = pd.read_csv('file.csv', parse_dates=['date_column'])
```

---

## ä½¿ç”¨æ–¹å¼

### é»˜è®¤ä½¿ç”¨

Skills é»˜è®¤è‡ªåŠ¨åŠ è½½ï¼š

```python
import loom
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="assistant",
    llm=OpenAILLM(api_key="...")
    # enable_skills=True    # âœ… é»˜è®¤
    # skills_dir="./skills" # âœ… é»˜è®¤
)

# 3 ä¸ªå†…ç½® Skills å·²åŠ è½½
```

### è‡ªå®šä¹‰ Skills ç›®å½•

```python
agent = loom.agent(
    name="assistant",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True,
    skills_dir="/path/to/my/skills"  # è‡ªå®šä¹‰è·¯å¾„
)
```

### ç¦ç”¨ Skills

```python
agent = loom.agent(
    name="assistant",
    llm=OpenAILLM(api_key="..."),
    enable_skills=False  # ç¦ç”¨æ‰€æœ‰ Skills
)
```

### åˆ—å‡º Skills

```python
# åˆ—å‡ºæ‰€æœ‰ Skills
skills = agent.list_skills()
for skill in skills:
    print(f"{skill.metadata.name}: {skill.metadata.description}")

# æŒ‰åˆ†ç±»ç­›é€‰
analysis_skills = agent.list_skills(category="analysis")
tool_skills = agent.list_skills(category="tools")
```

### æŸ¥çœ‹ Skill è¯¦æƒ…

```python
# è·å–ç‰¹å®š Skill
skill = agent.get_skill("pdf_analyzer")

print(f"Name: {skill.metadata.name}")
print(f"Description: {skill.metadata.description}")
print(f"Category: {skill.metadata.category}")
print(f"Version: {skill.metadata.version}")
print(f"Tags: {skill.metadata.tags}")

# åŠ è½½è¯¦ç»†æ–‡æ¡£
detailed_doc = skill.load_detailed_doc()
print(detailed_doc)

# æŸ¥çœ‹èµ„æºæ–‡ä»¶
resource_path = skill.get_resource_path("examples.json")
if resource_path:
    print(f"Resource: {resource_path}")
```

---

## å¸¸è§ç”¨ä¾‹

### ç”¨ä¾‹ 1ï¼šæ–‡æ¡£å¤„ç†å·¥ä½œæµ

```python
from loom import Message, SimpleAgent
from loom.builtin import OpenAILLM

agent = loom.agent(
    name="document-processor",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

# ä»»åŠ¡ï¼šå¤„ç†ä¸€æ‰¹å‘ç¥¨
msg = Message(
    role="user",
    content="""
    å¤„ç† invoices/ ç›®å½•ä¸‹çš„æ‰€æœ‰ PDF å‘ç¥¨ï¼š
    1. æå–æ¯å¼ å‘ç¥¨çš„ï¼šå…¬å¸åã€é‡‘é¢ã€æ—¥æœŸ
    2. æ±‡æ€»åˆ° Excel æ–‡ä»¶
    3. è®¡ç®—æ€»é‡‘é¢
    """
)

response = await agent.run(msg)

# Agent ä½¿ç”¨ï¼š
# - pdf_analyzer: æå– PDF æ•°æ®
# - data_processor: æ±‡æ€»åˆ° Excel
```

### ç”¨ä¾‹ 2ï¼šå¸‚åœºè°ƒç ”æŠ¥å‘Š

```python
agent = loom.agent(
    name="market-researcher",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

msg = Message(
    role="user",
    content="""
    åˆ›å»ºä¸€ä»½"AI Agent æ¡†æ¶"å¸‚åœºè°ƒç ”æŠ¥å‘Šï¼š
    1. æœç´¢å¹¶åˆ†æ 10 ç¯‡ç›¸å…³æ–‡ç« 
    2. æ€»ç»“ä¸»è¦è¶‹åŠ¿å’ŒæŒ‘æˆ˜
    3. å¯¹æ¯” 3 ä¸ªä¸»æµæ¡†æ¶
    4. ç”Ÿæˆ Markdown æŠ¥å‘Š
    """
)

response = await agent.run(msg)

# Agent ä½¿ç”¨ï¼š
# - web_research: æœç´¢å’ŒæŠ“å–æ–‡ç« 
# - data_processor: ç»“æ„åŒ–åˆ†æç»“æœ
```

### ç”¨ä¾‹ 3ï¼šæ•°æ®åˆ†æè‡ªåŠ¨åŒ–

```python
agent = loom.agent(
    name="data-analyst",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

msg = Message(
    role="user",
    content="""
    åˆ†æé”€å”®æ•°æ®ï¼š
    1. ä» sales_2024.xlsx è¯»å–æ•°æ®
    2. æŒ‰æœˆä»½å’Œäº§å“ç±»åˆ«èšåˆ
    3. è¯†åˆ«é”€å”®è¶‹åŠ¿
    4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼ˆæè¿°æ€§ï¼‰
    """
)

response = await agent.run(msg)

# Agent ä½¿ç”¨ï¼š
# - data_processor: Excel å¤„ç†å’Œèšåˆ
```

### ç”¨ä¾‹ 4ï¼šæ··åˆä»»åŠ¡

```python
agent = loom.agent(
    name="research-analyst",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

msg = Message(
    role="user",
    content="""
    å®Œæ•´çš„ç«å“åˆ†æï¼š
    1. ä»ç½‘ä¸Šæ”¶é›† 3 ä¸ªç«å“çš„ä¿¡æ¯
    2. ä¸‹è½½ä»–ä»¬çš„äº§å“ç™½çš®ä¹¦ï¼ˆPDFï¼‰
    3. æå–å…³é”®ç‰¹æ€§å’Œå®šä»·
    4. æ•´ç†åˆ°å¯¹æ¯”è¡¨ï¼ˆCSVï¼‰
    5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
)

response = await agent.run(msg)

# Agent ä½¿ç”¨æ‰€æœ‰ 3 ä¸ª Skillsï¼š
# - web_research: æ”¶é›†åœ¨çº¿ä¿¡æ¯
# - pdf_analyzer: åˆ†æç™½çš®ä¹¦
# - data_processor: æ•´ç†å¯¹æ¯”è¡¨
```

---

## å¯ç”¨å’Œç¦ç”¨

### è¿è¡Œæ—¶å¯ç”¨/ç¦ç”¨

```python
agent = loom.agent(
    name="assistant",
    llm=OpenAILLM(api_key="..."),
    enable_skills=True
)

# ç¦ç”¨ç‰¹å®š Skill
agent.disable_skill("web_research")
print(agent.list_skills())  # åªæ˜¾ç¤º pdf_analyzer å’Œ data_processor

# å¯ç”¨ Skill
agent.enable_skill("web_research")
print(agent.list_skills())  # æ˜¾ç¤ºæ‰€æœ‰ 3 ä¸ª Skills

# é‡æ–°åŠ è½½ Skillsï¼ˆä»ç£ç›˜ï¼‰
agent.reload_skills()
```

### æ°¸ä¹…ç¦ç”¨

ç¼–è¾‘ `skills/<skill_name>/skill.yaml`ï¼š

```yaml
metadata:
  name: web_research
  # ...
  enabled: false  # âœ… ç¦ç”¨
```

ç„¶åé‡æ–°åŠ è½½ï¼š
```python
agent.reload_skills()
```

### æŒ‰åœºæ™¯é€‰æ‹©

```python
# åœºæ™¯ 1ï¼šåªéœ€è¦æ–‡æ¡£å¤„ç†
agent = loom.agent(
    name="doc-processor",
    llm=llm,
    enable_skills=True
)
agent.disable_skill("web_research")
agent.disable_skill("data_processor")
# åªä¿ç•™ pdf_analyzer

# åœºæ™¯ 2ï¼šåªéœ€è¦æ•°æ®åˆ†æ
agent = loom.agent(
    name="data-analyst",
    llm=llm,
    enable_skills=True
)
agent.disable_skill("pdf_analyzer")
agent.disable_skill("web_research")
# åªä¿ç•™ data_processor
```

---

## æ•…éšœæ’é™¤

### Q1: Skills æ²¡æœ‰åŠ è½½ï¼Ÿ

**ç—‡çŠ¶**ï¼š`agent.list_skills()` è¿”å›ç©ºåˆ—è¡¨

**æ’æŸ¥æ­¥éª¤**ï¼š

1. æ£€æŸ¥ Skills ç›®å½•æ˜¯å¦å­˜åœ¨ï¼š
```bash
ls -la ./skills/
# åº”è¯¥çœ‹åˆ° pdf_analyzer/, web_research/, data_processor/
```

2. ç¡®è®¤ `enable_skills=True`ï¼š
```python
agent = loom.agent(
    name="assistant",
    llm=llm,
    enable_skills=True  # âœ… å¿…é¡»å¯ç”¨
)
```

3. æ£€æŸ¥ `skills_dir` è·¯å¾„ï¼š
```python
# ä½¿ç”¨ç»å¯¹è·¯å¾„
import os
skills_dir = os.path.abspath("./skills")
agent = loom.agent(
    name="assistant",
    llm=llm,
    skills_dir=skills_dir
)
```

### Q2: Agent æ²¡æœ‰ä½¿ç”¨ Skillï¼Ÿ

**ç—‡çŠ¶**ï¼šAgent å®Œæˆäº†ä»»åŠ¡ä½†æ²¡æœ‰ä½¿ç”¨ Skills

**å¯èƒ½åŸå› **ï¼š
1. quick_guide ä¸å¤Ÿæ˜ç¡®
2. ä»»åŠ¡æè¿°ä¸æ¸…æ¥š
3. Skill è¢«ç¦ç”¨

**è§£å†³æ–¹æ³•**ï¼š

```python
# 1. æ£€æŸ¥ Skill æ˜¯å¦å¯ç”¨
skills = agent.list_skills()
print([s.metadata.name for s in skills])

# 2. æ›´æ˜ç¡®çš„ä»»åŠ¡æè¿°
msg = Message(
    role="user",
    content="ä½¿ç”¨ pdf_analyzer skill æå– document.pdf çš„æ–‡æœ¬"  # âœ… æ˜ç¡®æåˆ° skill
)

# 3. æ£€æŸ¥ç³»ç»Ÿæç¤º
print(agent.system_prompt)
# åº”è¯¥åŒ…å« Skills ç´¢å¼•
```

### Q3: ä¾èµ–åŒ…æœªå®‰è£…ï¼Ÿ

**ç—‡çŠ¶**ï¼šAgent å°è¯•ä½¿ç”¨ Skill ä½†æŠ¥é”™

**è§£å†³æ–¹æ³•**ï¼š

```bash
# PDF Analyzer
pip install PyPDF2 pdfplumber

# Web Research
pip install requests beautifulsoup4

# Data Processor
pip install pandas openpyxl

# å…¨éƒ¨å®‰è£…
pip install PyPDF2 pdfplumber requests beautifulsoup4 pandas openpyxl
```

### Q4: å¦‚ä½•è°ƒè¯• Skill ä½¿ç”¨ï¼Ÿ

**ä½¿ç”¨äº‹ä»¶ç›‘æ§**ï¼š

```python
from loom.core.events import AgentEventType

def event_handler(event):
    if event.type == AgentEventType.AGENT_START:
        print(f"ğŸš€ Agent started")
    elif event.type == AgentEventType.TOOL_START:
        tool_name = event.data.get('tool_name')
        print(f"ğŸ”§ Calling tool: {tool_name}")
        # æ£€æŸ¥æ˜¯å¦åŒ…å« Bash å·¥å…·è°ƒç”¨ï¼ˆè¯»å– SKILL.mdï¼‰
        if tool_name == "bash":
            command = event.data.get('command', '')
            if 'skills/' in command:
                print(f"   ğŸ“– Reading Skill: {command}")

agent = loom.agent(
    name="assistant",
    llm=llm,
    enable_skills=True,
    event_handler=event_handler
)

msg = Message(role="user", content="åˆ†æ report.pdf")
response = await agent.run(msg)

# è¾“å‡ºä¼šæ˜¾ç¤º Agent æ˜¯å¦è¯»å–äº† SKILL.md
```

### Q5: Skill æ–‡æ¡£æ›´æ–°åæ²¡æœ‰ç”Ÿæ•ˆï¼Ÿ

**è§£å†³æ–¹æ³•**ï¼š

```python
# é‡æ–°åŠ è½½ Skills
agent.reload_skills()

# æˆ–è€…é‡æ–°åˆ›å»º Agent
agent = loom.agent(
    name="assistant",
    llm=llm,
    enable_skills=True
)
```

### Q6: å¦‚ä½•æŸ¥çœ‹ Skill ç»Ÿè®¡ä¿¡æ¯ï¼Ÿ

```python
# Agent ç»Ÿè®¡
stats = agent.get_stats()
print(stats)

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   'name': 'assistant',
#   'num_tools': 0,
#   'executor_stats': {...},
#   'skills': {
#       'total_skills': 3,
#       'enabled_skills': 3,
#       'disabled_skills': 0,
#       'categories': 2
#   }
# }
```

### Q7: Skills å ç”¨å¤ªå¤šä¸Šä¸‹æ–‡ï¼Ÿ

**ä¸ä¼šï¼è¿™å°±æ˜¯ä¸‰å±‚æ¶æ„çš„ä¼˜åŠ¿ï¼š**

```python
# æ£€æŸ¥ç³»ç»Ÿæç¤ºå¤§å°
prompt = agent.system_prompt
import tiktoken
encoder = tiktoken.get_encoding("cl100k_base")
tokens = len(encoder.encode(prompt))
print(f"System prompt tokens: {tokens}")

# 3 ä¸ª Skills çš„ç´¢å¼•åªå ç”¨çº¦ 150-200 tokens
# è¯¦ç»†æ–‡æ¡£åªåœ¨ Agent éœ€è¦æ—¶æ‰åŠ è½½ï¼ˆæŒ‰éœ€ï¼‰
```

---

## ä¸‹ä¸€æ­¥

### æ·±å…¥å­¦ä¹ 
- [Skills æ¦‚è¿°](./overview.md) - Skills ç³»ç»Ÿå®Œæ•´ä»‹ç»
- [åˆ›å»º Skills](./creating-skills.md) - è‡ªå®šä¹‰ Skills
- [Skills å¿«é€Ÿå‚è€ƒ](./quick-reference.md) - API é€ŸæŸ¥

### ç›¸å…³ä¸»é¢˜
- [SimpleAgent æŒ‡å—](../agents/simple-agent.md) - Agent å®Œæ•´åŠŸèƒ½
- [å·¥å…·å¼€å‘](../tools/development.md) - Tools vs Skills
- [äº‹ä»¶ç³»ç»Ÿ](../advanced/events.md) - ç›‘æ§ Skills ä½¿ç”¨

### ç¤ºä¾‹
- [ç¤ºä¾‹åº“](../../examples/) - å®Œæ•´ç¤ºä¾‹ä»£ç 
- [Skills æºç ](../../../skills/) - æŸ¥çœ‹å†…ç½® Skills å®ç°

---

## æ€»ç»“

**å†…ç½® Skills æ¦‚è§ˆ**ï¼š

| Skill | åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ | ä¸»è¦ä¾èµ– |
|-------|------|----------|----------|
| **pdf_analyzer** | PDF æ–‡æ¡£åˆ†æ | å‘ç¥¨ã€åˆåŒã€æŠ¥å‘Šå¤„ç† | PyPDF2, pdfplumber |
| **web_research** | Web ä¿¡æ¯æ”¶é›† | å¸‚åœºç ”ç©¶ã€ç«å“åˆ†æ | requests, beautifulsoup4 |
| **data_processor** | ç»“æ„åŒ–æ•°æ®å¤„ç† | æ•°æ®æ¸…æ´—ã€ETLã€åˆ†æ | pandas, openpyxl |

**å…³é”®è¦ç‚¹**ï¼š
1. **å¼€ç®±å³ç”¨**ï¼šSkills é»˜è®¤å¯ç”¨ï¼Œæ— éœ€é…ç½®
2. **ä¸‰å±‚æ¶æ„**ï¼šæœ€å°åŒ–ä¸Šä¸‹æ–‡ä½¿ç”¨ï¼ˆç´¢å¼• ~150 tokensï¼‰
3. **æŒ‰éœ€åŠ è½½**ï¼šAgent åªåœ¨éœ€è¦æ—¶è¯»å–è¯¦ç»†æ–‡æ¡£
4. **çµæ´»æ§åˆ¶**ï¼šå¯ä»¥å¯ç”¨/ç¦ç”¨ç‰¹å®š Skills
5. **æ˜“äºæ‰©å±•**ï¼šå¯ä»¥æ·»åŠ è‡ªå®šä¹‰ Skills

**è®°ä½**ï¼šSkills æ˜¯"çŸ¥è¯†"ï¼Œè®© Agent æ›´èªæ˜ï¼›Tools æ˜¯"èƒ½åŠ›"ï¼Œè®© Agent æ›´å¼ºå¤§ã€‚

---

**å¼€å§‹ä½¿ç”¨å†…ç½® Skills å§ï¼** ğŸš€
